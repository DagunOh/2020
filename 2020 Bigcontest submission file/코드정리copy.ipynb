{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 필요한 라이브러리 import & train, test read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel(\"../original_data/main_data.xlsx\")\n",
    "test = pd.read_excel(\"../original_data/template.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flag: if True -> train, else -> test\n",
    "# renaming\n",
    "def rename_valid(data):\n",
    "    data.rename(columns={\"방송일시\": \"broadDateTime\", \"노출(분)\": \"broadTime\",\n",
    "                         \"마더코드\": \"motherCode\", \"상품코드\": \"prodCode\", \"상품명\": \"prodName\",\n",
    "                         \"상품군\": \"prodGroup\", \"판매단가\": \"unitPrice\", \"취급액\": \"revenue\"}, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델링의 편의를 위해 한글로 된 변수 이름을 영어로 바꿔주는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadTime imputation\n",
    "def broadTime_imp(data):\n",
    "    na_broadTime_idx = list(data[data[\"broadTime\"].isna()].index)\n",
    "    max_num = 0\n",
    "    last_idx = -1\n",
    "    num = 1\n",
    "\n",
    "    for i, idx in enumerate(na_broadTime_idx):\n",
    "        if idx - last_idx == 1:\n",
    "            num += 1\n",
    "        else:\n",
    "            max_num = max(max_num, num)\n",
    "            num = 1\n",
    "\n",
    "        last_idx = idx\n",
    "\n",
    "    for idx in na_broadTime_idx:\n",
    "        max_num_idx = min(max_num, idx)\n",
    "        impute_val = data[idx - max_num_idx: idx][\"broadTime\"][~data[idx - max_num_idx: idx][\"broadTime\"].isna()].iloc[\n",
    "            -1]\n",
    "        data[\"broadTime\"].loc[idx] = impute_val\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        val = data[\"broadTime\"].iloc[i]\n",
    "        if 19 < val < 21:\n",
    "            val = 20\n",
    "        elif 29 < val < 31:\n",
    "            val = 30\n",
    "        else:\n",
    "            val = round(val)\n",
    "        data[\"broadTime\"].iloc[i] = val\n",
    "\n",
    "    data['broadTime'] = data['broadTime'].astype(int)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "broadTime, 즉 방송시간이 nan인 것들을 imputation 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nas\n",
    "def na_dropping(data, flag=True):\n",
    "    if flag:\n",
    "        data = data[data[\"revenue\"] != 50000]\n",
    "        data = data[~data[\"revenue\"].isna()]\n",
    "        data = data[data[\"prodGroup\"] != \"무형\"]\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "    else:\n",
    "        data['revenue'] = 10 ** 5 # dummy\n",
    "        data = data[data[\"prodGroup\"] != \"무형\"]\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flag=True는 트레인 데이터를 의미하고 flag=False는 테스트 테이터를 의미합니다. <br>\n",
    "트레인시에는 무형인 데이터와 revenue가 5만인 데이터를 빼주고, 테스트시에는 무형인 데이터를 빼줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_transformation(data):\n",
    "    data[\"target\"] = np.divide(data[\"revenue\"], data[\"unitPrice\"]).astype(np.float64)\n",
    "    data[\"target\"] = np.divide(data[\"target\"], data[\"broadTime\"]).astype(np.float64)\n",
    "    data[\"target\"] = np.log(data[\"target\"])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target을 revenue에서 log(분당판매량((revenue/unitPrice)/broadTime))으로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나로 합치기\n",
    "def first_eda(data, flag=True):\n",
    "    dat = rename_valid(data)\n",
    "    dat = broadTime_imp(dat)\n",
    "    dat = na_dropping(dat, flag=flag)\n",
    "    dat = target_transformation(dat)\n",
    "\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 모든 과정을 하나로 정리한 함수입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = first_eda(train)\n",
    "test = first_eda(test, flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "적용시켜줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new variables\n",
    "def add_newvars(data):\n",
    "    # gender\n",
    "    isFemale = [1 if \"여성\" in name or \"여자\" in name else 0 for name in data.prodName]\n",
    "    #isMale = [1 if \"남성\" in name or \"남자\" in name else 0 for name in data.prodName]\n",
    "    data[\"isFemale\"] = isFemale\n",
    "    #data[\"isMale\"] = isMale\n",
    "\n",
    "    # 할부\n",
    "    paymentPlan = [1 if \"무이자\" in name or \"(무)\" in name else 0 for name in data.prodName]\n",
    "    data[\"paymentPlan\"] = paymentPlan\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 변수: 성별변수와 할부여부를 넣어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change dayofweek: 0, 1, 2시 인 경우 전날로 바꿔줌\n",
    "def change_dayofWeek(data):\n",
    "    #dayofWeek\n",
    "    broadDateTime = pd.to_datetime(data['broadDateTime'])\n",
    "\n",
    "    day_lst_f = []\n",
    "\n",
    "    for i in range(broadDateTime.shape[0]):\n",
    "        year = broadDateTime[i].year\n",
    "        month = broadDateTime[i].month\n",
    "        day = broadDateTime[i].day\n",
    "        hour = broadDateTime[i].hour\n",
    "        broadDayofWeek = broadDateTime[i].weekday()\n",
    "\n",
    "        if hour== 0 or hour ==1 or hour==2 :\n",
    "            today = datetime(year,month,day)\n",
    "            yes = today - timedelta(days = 1)\n",
    "            yes = yes.weekday()\n",
    "            day_lst_f.append(yes)\n",
    "        else:\n",
    "            day_lst_f.append(broadDayofWeek)\n",
    "\n",
    "    data['broadDayOfWeek'] = day_lst_f\n",
    "    return data\n",
    "\n",
    "#change time: 0시->24시, 1시->25시, 2시->26시\n",
    "def change_time(data):\n",
    "    day_lst = []\n",
    "    broadDateTime = pd.to_datetime(data['broadDateTime'])\n",
    "\n",
    "    for i in range(broadDateTime.shape[0]):\n",
    "        year = broadDateTime[i].year\n",
    "        month = broadDateTime[i].month\n",
    "        day = broadDateTime[i].day\n",
    "        hour = broadDateTime[i].hour\n",
    "        minute = broadDateTime[i].minute\n",
    "        second = broadDateTime[i].second\n",
    "        today = datetime(year, month, day)\n",
    "        yes = today - timedelta(days = 1)\n",
    "\n",
    "        if hour== 0 : hour = '24'\n",
    "        elif hour == 1 : hour = '25'\n",
    "        elif hour == 2 : hour = '26'\n",
    "\n",
    "        if hour in ['24', '25', '26'] :\n",
    "            yesterday = str(yes.year) + '-' +str(yes.month)+ '-'+str(yes.day)\n",
    "            then = str(hour) + ':' +str(minute)+ ':'+ str(second)\n",
    "            day_lst.append(yesterday+' '+then)\n",
    "        else:\n",
    "            day_lst.append(str(broadDateTime[i]))\n",
    "\n",
    "    #broadDateTime 바꾸고\n",
    "    data['broadDateTime'] =day_lst\n",
    "\n",
    "    #각각 파싱\n",
    "    lst_Y=[]\n",
    "    lst_M = []\n",
    "    lst_D =[]\n",
    "    lst_H = []\n",
    "    lst_Mi = []\n",
    "    for i in range(len(day_lst)):\n",
    "        broadYear = str(data['broadDateTime'][i]).split('-')[0]\n",
    "        broadMonth = str(data['broadDateTime'][i]).split('-')[1]\n",
    "        broadDay = str(data['broadDateTime'][i]).split('-')[2].split(' ')[0]\n",
    "        broadHour = str(data['broadDateTime'][i]).split('-')[2].split(' ')[1].split(':')[0]\n",
    "        broadMin = str(data['broadDateTime'][i]).split('-')[2].split(' ')[1].split(':')[1]\n",
    "        lst_Y.append(int(broadYear))\n",
    "        lst_M.append(int(broadMonth))\n",
    "        lst_D.append(int(broadDay))\n",
    "        lst_H.append(int(broadHour))\n",
    "        lst_Mi.append(int(broadMin))\n",
    "\n",
    "    data.insert(1, \"broadYear\", lst_Y)\n",
    "    data.insert(2, \"broadMonth\", lst_M)\n",
    "    data.insert(3, \"broadDay\", lst_D)\n",
    "    data.insert(4, \"broadHour\", lst_H)\n",
    "    data.insert(5, \"broadMin\", lst_Mi)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0시, 1시, 2시는 새벽이므로 그 전날과 더 가깝다고 보아 날짜를 바꿔주고, 혼동의 여지가 없도록 0시를 24시로 1시를 25시로 <br> 2시를 26시로 바꿔주는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of products in same time\n",
    "def prodCount(data):\n",
    "    Map=dict()\n",
    "    data['prodCount']=0\n",
    "    for x in data['broadDateTime'] :\n",
    "        time = str(x)\n",
    "        if time not in Map:\n",
    "            Map[time] = 1\n",
    "        else:\n",
    "            Map[time] +=1\n",
    "        data['prodCount'][data['broadDateTime']==time]=Map[time]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 방송시간대당 몇개의 상품을 판매했는지를 나타내는 prodCount변수를 새로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holidays\n",
    "def holidays(data):\n",
    "    holidays= []\n",
    "#일요일인 국가 지정 공휴일은 제외\n",
    "    holiday_lst = ['2019-01-01','2019-02-04','2019-02-05','2019-02-06',\n",
    "              '2019-03-01','2019-05-06','2019-06-06','2019-08-15',\n",
    "              '2019-09-12', '2019-09-13','2019-09-14', '2019-10-03','2019-10-09',\n",
    "              '2019-12-25', '2020-06-06', '2019-1-1','2019-2-4','2019-2-5','2019-2-6',\n",
    "              '2019-3-1','2019-5-6','2019-6-6','2019-8-15',\n",
    "              '2019-9-12', '2019-9-13','2019-9-14', '2019-10-3','2019-10-9',\n",
    "              '2019-12-25', '2020-6-6']\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "    #holiday list 속 애들 - append(1)\n",
    "        if (data.broadDateTime[i].split(' ')[0] in holiday_lst) == True:\n",
    "            holidays.append(1)\n",
    "    #holiday list 속 애들이 아니면서 dayofWeek이 6인 것 (일요일인것)\n",
    "        elif data.broadDayOfWeek[i] ==6:\n",
    "            holidays.append(1)\n",
    "    #쟤도 걔도 아닌것.\n",
    "        else:\n",
    "            holidays.append(0)\n",
    "\n",
    "    data['isHoliday'] = holidays\n",
    "\n",
    "    ## holiday len ##\n",
    "\n",
    "    data[\"isHoliday\"][data[\"broadDayOfWeek\"] == 5] = 1\n",
    "    data[\"isHoliday\"][data[\"broadDayOfWeek\"] == 6] = 1\n",
    "\n",
    "    grouped = data.groupby([\"broadMonth\", \"broadDay\"]).mean().reset_index()\n",
    "    grouped = grouped[[\"broadMonth\", \"broadDay\", \"isHoliday\"]]\n",
    "\n",
    "    holiday_len = []\n",
    "    cnt = 0\n",
    "    for holiday in grouped[\"isHoliday\"]:\n",
    "        if holiday == 1:\n",
    "            cnt += 1\n",
    "        elif holiday == 0:\n",
    "            for _ in range(cnt):\n",
    "                holiday_len.append(cnt)\n",
    "            cnt = 0\n",
    "            holiday_len.append(cnt)\n",
    "\n",
    "    grouped[\"holidayLen\"] = holiday_len\n",
    "    data = data.merge(grouped)\n",
    "    data.drop(\"isHoliday\", inplace = True, axis = 1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연속으로 몇 번째 공휴일인지 나타내는 변수 holidayLen을 만드는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add index\n",
    "def add_index(data):\n",
    "    data['unnamed'] = data.index\n",
    "\n",
    "    # 비교군 2개\n",
    "    li_time1 = list(map(lambda x: '0'+str(x) if len(str(x)) == 1 else x, data['broadMonth'].values.tolist()))\n",
    "    li_time2 = list(map(lambda x: '0'+str(x) if len(str(x)) == 1 else x, data['broadDay'].values.tolist()))\n",
    "    li_time = [str(x)+str(y) for x, y in zip(li_time1, li_time2)]\n",
    "\n",
    "    data['time_index'] = li_time\n",
    "    data = data.sort_values(['prodCode', 'time_index'])\n",
    "\n",
    "    li_prod = data['prodCode'].values.tolist()\n",
    "    li_time = data['time_index'].values.tolist()\n",
    "\n",
    "    n=1\n",
    "    index =[]\n",
    "    index.append(n)\n",
    "\n",
    "    for i in range(len(data)-1):\n",
    "        if li_prod[i] != li_prod[i+1]: # 다른 상품이면 n 초기화\n",
    "            n = 1\n",
    "            index.append(n)\n",
    "            continue\n",
    "        if li_time[i] != li_time[i+1]: # 같은 상품에, 연속된거 아니면 n 초기화\n",
    "            n = 1\n",
    "            index.append(n)\n",
    "            continue\n",
    "\n",
    "        n += 1\n",
    "        index.append(n)\n",
    "\n",
    "    data['prod_index'] = index\n",
    "\n",
    "    tmp = data.groupby(['prodCode', 'broadMonth', 'broadDay']).max()['prod_index'].reset_index()\n",
    "    tmp = pd.merge(data, tmp, how='left', on=['prodCode', 'broadMonth', 'broadDay'])\n",
    "    tmp.sort_values('unnamed', inplace=True)\n",
    "    tmp.rename(columns={'prod_index_y': 'max_index',\n",
    "                        'prod_index_x': 'prod_index'}, inplace=True)\n",
    "    tmp.drop(['time_index', 'unnamed'], axis=1, inplace=True)\n",
    "\n",
    "    ## start index & end index\n",
    "\n",
    "    tmp[\"start_index\"], tmp[\"end_index\"] = np.zeros(tmp.shape[0]), np.zeros(tmp.shape[0])\n",
    "    tmp[\"start_index\"][tmp[\"prod_index\"] == 1] = 1\n",
    "    tmp[\"end_index\"][tmp[\"prod_index\"] == tmp[\"max_index\"]] = 1\n",
    "\n",
    "    tmp[[\"start_index\", \"end_index\"]] = tmp[[\"start_index\", \"end_index\"]].astype(\"int\")\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연속된 방송중 몇 번째 방송인지 나타내주는 변수 prod_index를 만들고, start_index는 시작 방송인지 max_index는 마지막 방송인지를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6월인지 아닌지 / test는 무조건 1\n",
    "def isJune(data, flag=True):\n",
    "    if flag:\n",
    "        data[\"isJune\"] = np.zeros(data.shape[0])\n",
    "        data[\"isJune\"][data[\"broadMonth\"] == 6] = 1\n",
    "    else:\n",
    "        data[\"isJune\"] = 1\n",
    "\n",
    "    data[\"isJune\"] = data[\"isJune\"].astype('int')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 위해 6월을 구분하는 함수를 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조기품절되었는지\n",
    "def stop(data):\n",
    "    timeMap = dict()\n",
    "    for month, day, maxIndex, code,prod, time in zip(data['broadMonth'], data['broadDay'], data[\"max_index\"],data['prodCode'],data[\"prodName\"],data[\"broadTime\"]):\n",
    "        name = str(month)+str(day)+str(code)+str(prod) + str(maxIndex)\n",
    "        if name not in timeMap:\n",
    "            timeMap[name] = time\n",
    "        else:\n",
    "            if timeMap[name] < time:\n",
    "                timeMap[name] = time\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    def make_stop(month, day, maxIndex, code, prod, time):\n",
    "        name = str(month)+str(day)+str(code)+str(prod) + str(maxIndex)\n",
    "        return time/timeMap[name]\n",
    "\n",
    "    data['stop'] = data.apply(lambda row: make_stop(row['broadMonth'], row['broadDay'],row[\"max_index\"], row['prodCode'],row[\"prodName\"], row[\"broadTime\"]), axis=1)\n",
    "    data['stop'] = 1/data['stop']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조기품절되었는지의 여부를 나타내주는 변수 stop입니다. 즉, 원래의 방송시간보다 짧게 방송되었다는 의미입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제품명에 이름이 들어갔는지\n",
    "def isNamed(data):\n",
    "    names = {'전지현', '팽현숙', '전철우', '강레오', '김선영', '김정문', '김정배',\n",
    "             '김규흔', '이봉원', '오세득', '유귀열', '이경제', '이만기', '이보은', '임성근', '최인선',\n",
    "             '김병만', '김병지', '이정섭', '이동수', '서장훈', '송도순', '효재', '천수봉', '숀리',\n",
    "             '임화자', 'aab', 'aac'}\n",
    "\n",
    "    data['isNamed'] = 0\n",
    "    for i, prod_name in enumerate(data.prodName.values.tolist()):\n",
    "        for name in names:\n",
    "            if name in prod_name.lower():\n",
    "                data.isNamed.iloc[i] = 1\n",
    "                break\n",
    "\n",
    "    data['isNamed'] = data['isNamed'].astype(int)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "named 제품인지를 나타내주는 변수 isNamed를 만드는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 월급날 변수\n",
    "def isPayday(data):\n",
    "    payday={10, 11, 12, 13, 20, 21, 22, 23, 25, 26, 27, 28}\n",
    "    isPayday = [1 if day in payday else 0 for day in data.broadDay]\n",
    "    data['isPayday'] = isPayday\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "월급날 여부를 알려주는 isPayday 변수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부시청률: 방영 전 시간대 인기많은 프로그램의 갯수\n",
    "def isPre(train, broad_final):\n",
    "    tmp = np.zeros((train.shape[0],), dtype=int)\n",
    "    for i, row in train.iterrows():\n",
    "        broad_new = broad_final[broad_final.month == row.loc['broadMonth']]\n",
    "        broad_new = broad_new[broad_new.day == row.loc['broadDay']]\n",
    "        broad_new = broad_new[broad_new.start_hour == row.loc['broadHour']-1]\n",
    "        tmp[i] = len(broad_new)\n",
    "    return tmp\n",
    "\n",
    "# 외부시청률: 방영 시간대 인기많은 프로그램의 갯수\n",
    "def isNow(train, broad_final):\n",
    "    tmp = np.zeros((train.shape[0],), dtype=int)\n",
    "    for i, row in train.iterrows():\n",
    "        broad_new = broad_final[broad_final.month == row.loc['broadMonth']]\n",
    "        broad_new = broad_new[broad_new.day == row.loc['broadDay']]\n",
    "        broad_new = broad_new[broad_new.start_hour == row.loc['broadHour']]\n",
    "        tmp[i] = len(broad_new)\n",
    "    return tmp\n",
    "\n",
    "# 외부시청률: 방영 후 시간대 인기많은 프로그램의 갯수\n",
    "def isNext(train, broad_final):\n",
    "    tmp = np.zeros((train.shape[0],), dtype=int)\n",
    "    for i, row in train.iterrows():\n",
    "        broad_new = broad_final[broad_final.month == row.loc['broadMonth']]\n",
    "        broad_new = broad_new[broad_new.day == row.loc['broadDay']]\n",
    "        broad_new = broad_new[broad_new.end_hour == row.loc['broadHour']+1]\n",
    "        tmp[i] = len(broad_new)\n",
    "    return tmp\n",
    "\n",
    "# 시청률 변수 넣기\n",
    "def add_rating(data, flag=True):\n",
    "    if flag:\n",
    "        file_names = ['broad_lst1_new.csv', 'broad_lst2_new_no_re.csv', 'broad_lst3_new.csv',\n",
    "                      'broad_lst4_new.csv', 'broad_lst5_new.csv']\n",
    "        file_paths = tuple(map(lambda x: os.path.join('../eda/data/rating_csv/', x), file_names))\n",
    "\n",
    "        files = {}\n",
    "        for file_name, file_path in zip(file_names, file_paths):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                df = df[['title', 'dayOfWeek', 'start_hour', 'start_min', 'end_hour', 'end_min']]\n",
    "                df.rename(columns={'dayOfweek': 'dayOfWeek'}, inplace=True)\n",
    "            except:\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                df.columns = ['title', 'dayOfWeek', 'start_hour', 'start_min', 'end_hour', 'end_min']\n",
    "            if file_name == 'broad_lst5_new.csv':\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "                df.columns = ['title', 'start_hour', 'start_min', 'end_hour', 'end_min', 'dayOfWeek']\n",
    "                df = df[['title', 'dayOfWeek', 'start_hour', 'start_min', 'end_hour', 'end_min']]\n",
    "\n",
    "            files[file_name[:10]] = df\n",
    "\n",
    "        broad_lst = pd.concat([*files.values()], axis=0)\n",
    "        broad_final = pd.read_csv('../eda/data/rating_csv/broad_final_added.csv', index_col=0)\n",
    "\n",
    "        broad_final_year = [str(x) for x in broad_final.year.values.tolist()]\n",
    "        broad_final_month = ['0' + str(x) if len(str(x)) < 2 else str(x)\n",
    "                             for x in broad_final.month.values.tolist()]\n",
    "        broad_final_day = ['0' + str(x) if len(str(x)) < 2 else str(x)\n",
    "                           for x in broad_final.day.values.tolist()]\n",
    "\n",
    "        broadDateTime = list(map(lambda x, y, z: x + y + z, broad_final_year, broad_final_month,\n",
    "                                 broad_final_day))\n",
    "        broadDateTime = pd.to_datetime(broadDateTime, utc=False)\n",
    "        broadDayOfWeek = [x.weekday() for x in broadDateTime]\n",
    "\n",
    "        broad_final.insert(3, \"dayOfWeek\", broadDayOfWeek)\n",
    "        broad_final = broad_final.drop(['start_hour', 'start_min', 'end_hour', 'end_min'], axis=1)\n",
    "        broad_final = pd.merge(broad_final, broad_lst, on=['dayOfWeek', 'title'])\n",
    "\n",
    "        broad_final.loc[2272, 'day'] = 14\n",
    "        broad_final.loc[2278, 'day'] = 14\n",
    "\n",
    "    else:\n",
    "        file_path = os.path.join('../eda/data/rating_csv/', 'broad_lst_2020_new.csv')\n",
    "\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        df.columns = ['title', 'start_hour', 'start_min', 'end_hour', 'end_min', 'dayOfWeek']\n",
    "        df = df[['title', 'dayOfWeek', 'start_hour', 'start_min', 'end_hour', 'end_min']]\n",
    "\n",
    "        broad_lst = df\n",
    "        broad_final = pd.read_csv('../eda/data/rating_csv/broad_final_2020.csv', index_col=0)\n",
    "\n",
    "        broad_final_year = [str(x) for x in broad_final.year.values.tolist()]\n",
    "        broad_final_month = ['0' + str(x) if len(str(x)) < 2 else str(x)\n",
    "                             for x in broad_final.month.values.tolist()]\n",
    "        broad_final_day = ['0' + str(x) if len(str(x)) < 2 else str(x)\n",
    "                           for x in broad_final.day.values.tolist()]\n",
    "\n",
    "        broadDateTime = list(map(lambda x, y, z: x + y + z, broad_final_year, broad_final_month,\n",
    "                                 broad_final_day))\n",
    "        broadDateTime = pd.to_datetime(broadDateTime, utc=False)\n",
    "        broadDayOfWeek = [x.weekday() for x in broadDateTime]\n",
    "\n",
    "        broad_final.insert(3, \"dayOfWeek\", broadDayOfWeek)\n",
    "        broad_final = broad_final.drop(['start_hour', 'start_min', 'end_hour', 'end_min'], axis=1)\n",
    "        broad_final = pd.merge(broad_final, broad_lst, on=['dayOfWeek', 'title'])\n",
    "\n",
    "    pre = isPre(data, broad_final)\n",
    "    now = isNow(data, broad_final)\n",
    "    next = isNext(data, broad_final)\n",
    "    data['rating'] = list(map(lambda x, y, z: max((x, y, z)), pre, now, next))\n",
    "    data['rating'] = data['rating'].astype('category')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "외부에서 크롤링한 시청률 데이터를 이용해 전시간대/방영시간대/후시간대에 인기 프로그램(시청률 8.5 이상) 갯수를 각각 구하고 <br> 3개중 max값을 rating변수로 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional data\n",
    "scaler = MinMaxScaler()\n",
    "def add_exdata(data):\n",
    "    data.insert(1, \"no\", range(0, len(data)))\n",
    "\n",
    "    # priceIndex Variable\n",
    "    pi = pd.read_excel(\"../eda/data/priceIndex_2015100__201901-202007.xlsx\", header=0)\n",
    "    pi=pi[['시점','식품','식품 이외']] # 전국 생활물가지수 사용\n",
    "\n",
    "    broadYear=[int(i[0:4]) for i in pi['시점']]\n",
    "    broadMonth=[int(i[6:8]) for i in pi['시점']]\n",
    "    pi.insert(1,'broadYear',broadYear)\n",
    "    pi.insert(2,'broadMonth',broadMonth)\n",
    "    pi.drop('시점',axis=1,inplace=True)\n",
    "\n",
    "    foodIndex=[108.79-108.45]\n",
    "    for i in range(len(pi)-1):\n",
    "        delta=pi['식품'][i+1]-pi['식품'][i]\n",
    "        foodIndex.append(delta)\n",
    "\n",
    "    nonfoodIndex=[101.49-102.12]\n",
    "\n",
    "    for i in range(len(pi)-1):\n",
    "        delta2=pi['식품 이외'][i+1]-pi['식품 이외'][i]\n",
    "        nonfoodIndex.append(delta2)\n",
    "\n",
    "    pi['foodIndex']=foodIndex\n",
    "    pi['nonfoodIndex']=nonfoodIndex\n",
    "\n",
    "    data_food=data[(data['prodGroup']=='농수축')|(data['prodGroup']=='건강기능')]\n",
    "    data_food=pd.merge(data_food,pi[['broadYear','broadMonth','foodIndex']],\n",
    "                   on=['broadYear','broadMonth'])\n",
    "    data_food.rename(columns={'foodIndex':'priceIndex'},inplace=True)\n",
    "    data_nonfood=data[~((data['prodGroup']=='농수축')|\n",
    "                    (data['prodGroup']=='건강기능'))]\n",
    "    data_nonfood=pd.merge(data_nonfood,pi[['broadYear','broadMonth','nonfoodIndex']],\n",
    "                      on=['broadYear','broadMonth'])\n",
    "    data_nonfood.rename(columns={'nonfoodIndex':'priceIndex'},inplace=True)\n",
    "    data=pd.concat([data_food,data_nonfood])\n",
    "\n",
    "    # tempNorm Variable\n",
    "    temper = pd.read_csv('../eda/data/temper2.csv', header=6, encoding='cp949')\n",
    "\n",
    "    temper[\"날짜\"] = pd.to_datetime(temper[\"날짜\"])\n",
    "\n",
    "    broadYear = temper[\"날짜\"].dt.year\n",
    "    broadMonth = temper[\"날짜\"].dt.month\n",
    "    broadDay = temper[\"날짜\"].dt.day\n",
    "    temper.insert(1, \"broadYear\", broadYear)\n",
    "    temper.insert(2, \"broadMonth\", broadMonth)\n",
    "    temper.insert(3, \"broadDay\", broadDay)\n",
    "    temper.drop(\"날짜\", axis=1, inplace=True)\n",
    "\n",
    "    data = pd.merge(data, temper[['broadYear', 'broadMonth',\n",
    "                                  'broadDay', '평균기온(℃)']],\n",
    "                    on=['broadYear', 'broadMonth', 'broadDay'])\n",
    "    data.rename(columns={'평균기온(℃)': 'temperature'}, inplace=True)\n",
    "\n",
    "    m = pd.DataFrame()\n",
    "\n",
    "    for i in data['broadMonth'].unique():\n",
    "        dataMonth = data[data['broadMonth'] == i]\n",
    "        dataMonth['tempNorm'] = scaler.fit_transform(dataMonth[['temperature']])\n",
    "        m = pd.concat([m, dataMonth])\n",
    "\n",
    "    data = m.drop('temperature', axis=1)\n",
    "\n",
    "\n",
    "    # rainAvgAll, rainAvgCap Variable(강수량)\n",
    "    rain_all = pd.read_excel('../eda/data/rain_all.xlsx', header=13)\n",
    "    rain_cap = pd.read_excel('../eda/data/rain_cap.xlsx', header=13)\n",
    "\n",
    "    rain_all = rain_all[['일시', '평균일강수량(mm)']]\n",
    "    rain_cap = rain_cap[['일시', '평균일강수량(mm)']]\n",
    "\n",
    "    rain_all = rain_all[~rain_all.일시.isnull()]\n",
    "    rain_cap = rain_cap[~rain_cap.일시.isnull()]\n",
    "\n",
    "    broadYear = rain_all[\"일시\"].dt.year\n",
    "    broadMonth = rain_all[\"일시\"].dt.month\n",
    "    broadDay = rain_all[\"일시\"].dt.day\n",
    "    rain_all.insert(1, \"broadYear\", broadYear)\n",
    "    rain_all.insert(2, \"broadMonth\", broadMonth)\n",
    "    rain_all.insert(3, \"broadDay\", broadDay)\n",
    "    rain_all.drop('일시', axis=1, inplace=True)\n",
    "\n",
    "    broadYear = rain_cap[\"일시\"].dt.year\n",
    "    broadMonth = rain_cap[\"일시\"].dt.month\n",
    "    broadDay = rain_cap[\"일시\"].dt.day\n",
    "    rain_cap.insert(1, \"broadYear\", broadYear)\n",
    "    rain_cap.insert(2, \"broadMonth\", broadMonth)\n",
    "    rain_cap.insert(3, \"broadDay\", broadDay)\n",
    "    rain_cap.drop('일시', axis=1, inplace=True)\n",
    "\n",
    "    rain_all2019 = rain_all[rain_all.broadYear == 2019]\n",
    "    rain_all202006 = rain_all[(rain_all.broadYear == 2020) & (rain_all.broadMonth == 6)]\n",
    "    rain_cap2019 = rain_cap[rain_cap.broadYear == 2019]\n",
    "    rain_cap202006 = rain_cap[(rain_cap.broadYear == 2020) & (rain_cap.broadMonth == 6)]\n",
    "\n",
    "    scalerAll = MinMaxScaler()\n",
    "    scalerAll.fit(rain_all2019[['평균일강수량(mm)']])\n",
    "    scalerCap = MinMaxScaler()\n",
    "    scalerCap.fit(rain_cap2019[['평균일강수량(mm)']])\n",
    "\n",
    "    rain_all['평균일강수량(mm)'] = scalerAll.transform(rain_all[['평균일강수량(mm)']])\n",
    "    rain_cap['평균일강수량(mm)'] = scalerCap.transform(rain_cap[['평균일강수량(mm)']])\n",
    "\n",
    "    data = pd.merge(data, rain_all[['broadYear', 'broadMonth', 'broadDay',\n",
    "                                    '평균일강수량(mm)']],\n",
    "                    on=['broadYear', 'broadMonth', 'broadDay'])\n",
    "    data.rename(columns={\"평균일강수량(mm)\": \"rainAvgWhole\"},\n",
    "                inplace=True)\n",
    "\n",
    "    data = pd.merge(data, rain_cap[['broadYear', 'broadMonth', 'broadDay',\n",
    "                                    '평균일강수량(mm)']],\n",
    "                    on=['broadYear', 'broadMonth', 'broadDay'])\n",
    "    data.rename(columns={\"평균일강수량(mm)\": \"rainAvgCap\"},\n",
    "                inplace=True)\n",
    "\n",
    "    #dust Variable: 미세먼지\n",
    "\n",
    "    dust1_2=pd.read_excel('../eda/data/dust/19-1미세먼지2.5.xlsx',header=4)\n",
    "    dust1_10=pd.read_excel('../eda/data/dust/19-1미세먼지10.xlsx',header=4)\n",
    "    dust2_2=pd.read_excel('../eda/data/dust/19-2미세먼지2.5.xlsx',header=4)\n",
    "    dust2_10=pd.read_excel('../eda/data/dust/19-2미세먼지10.xlsx',header=4)\n",
    "    dust3_2=pd.read_excel('../eda/data/dust/19-3미세먼지2.5.xlsx',header=4)\n",
    "    dust3_10=pd.read_excel('../eda/data/dust/19-3미세먼지10.xlsx',header=4)\n",
    "    dust4_2=pd.read_excel('../eda/data/dust/19-4미세먼지2.5.xlsx',header=4)\n",
    "    dust4_10=pd.read_excel('../eda/data/dust/19-4미세먼지10.xlsx',header=4)\n",
    "    dust5_2=pd.read_excel('../eda/data/dust/19-5미세먼지2.5.xlsx',header=4)\n",
    "    dust5_10=pd.read_excel('../eda/data/dust/19-5미세먼지10.xlsx',header=4)\n",
    "    dust6_2=pd.read_excel('../eda/data/dust/19-6미세먼지2.5.xlsx',header=4)\n",
    "    dust6_10=pd.read_excel('../eda/data/dust/19-6미세먼지10.xlsx',header=4)\n",
    "    dust7_2=pd.read_excel('../eda/data/dust/19-7미세먼지2.5.xlsx',header=4)\n",
    "    dust7_10=pd.read_excel('../eda/data/dust/19-7미세먼지10.xlsx',header=4)\n",
    "    dust8_2=pd.read_excel('../eda/data/dust/19-8미세먼지2.5.xlsx',header=4)\n",
    "    dust8_10=pd.read_excel('../eda/data/dust/19-8미세먼지10.xlsx',header=4)\n",
    "    dust9_2=pd.read_excel('../eda/data/dust/19-9미세먼지2.5.xlsx',header=4)\n",
    "    dust9_10=pd.read_excel('../eda/data/dust/19-9미세먼지10.xlsx',header=4)\n",
    "    dust10_2=pd.read_excel('../eda/data/dust/19-10미세먼지2.5.xlsx',header=4)\n",
    "    dust10_10=pd.read_excel('../eda/data/dust/19-10미세먼지10.xlsx',header=4)\n",
    "    dust11_2=pd.read_excel('../eda/data/dust/19-11미세먼지2.5.xlsx',header=4)\n",
    "    dust11_10=pd.read_excel('../eda/data/dust/19-11미세먼지10.xlsx',header=4)\n",
    "    dust12_2=pd.read_excel('../eda/data/dust/19-12미세먼지2.5.xlsx',header=4)\n",
    "    dust12_10=pd.read_excel('../eda/data/dust/19-12미세먼지10.xlsx',header=4)\n",
    "    dust206_2=pd.read_excel('../eda/data/dust/20-6미세먼지2.5.xlsx',header=4)\n",
    "    dust206_10=pd.read_excel('../eda/data/dust/20-6미세먼지10.xlsx',header=4)\n",
    "\n",
    "    lst2019=[dust1_2,\n",
    "     dust2_2,\n",
    "     dust3_2,\n",
    "     dust4_2,\n",
    "     dust5_2,\n",
    "     dust6_2,\n",
    "     dust7_2,\n",
    "     dust8_2,\n",
    "     dust9_2,\n",
    "     dust10_2,\n",
    "     dust11_2,\n",
    "     dust12_2,\n",
    "     dust1_10,\n",
    "     dust2_10,\n",
    "     dust3_10,\n",
    "     dust4_10,\n",
    "     dust5_10,\n",
    "     dust6_10,\n",
    "     dust7_10,\n",
    "     dust8_10,\n",
    "     dust9_10,\n",
    "     dust10_10,\n",
    "     dust11_10,\n",
    "     dust12_10,\n",
    "            ]\n",
    "\n",
    "    dust2019=pd.DataFrame(columns=['broadYear','broadMonth','broadDay','dust2.5','dust10'])\n",
    "    for i in range(12):\n",
    "        lst2019[i]=pd.DataFrame(lst2019[i].iloc[0,2:]).reset_index().rename(\n",
    "            columns={'index':'broadDay',0:'dust2.5'})\n",
    "        lst2019[i]['broadMonth']=i+1\n",
    "        lst2019[i+12]=pd.DataFrame(lst2019[i+12].iloc[0,2:]).reset_index().rename(\n",
    "            columns={'index':'broadDay',0:'dust10'})\n",
    "        lst2019[i+12]['broadMonth']=i+1\n",
    "        dust2019=pd.concat([dust2019,(pd.merge(lst2019[i],lst2019[i+12],on=['broadMonth','broadDay']))])\n",
    "\n",
    "    dust2019['broadDay']=dust2019['broadDay'].str.rstrip('일')\n",
    "    dust2019['broadDay']=dust2019['broadDay'].astype('int')\n",
    "    dust2019['broadYear']=2019\n",
    "    dust2019.dropna(axis=0,inplace=True) # 없는 날짜가 존재하는 행 삭제\n",
    "\n",
    "    dust202006=pd.DataFrame(columns=['broadYear','broadMonth','broadDay','dust2.5','dust10'])\n",
    "\n",
    "    dust206_2=pd.DataFrame(dust206_2.iloc[0,2:]).reset_index().rename(\n",
    "        columns={'index':'broadDay',0:'dust2.5'})\n",
    "    dust206_2['broadMonth']=6\n",
    "    dust206_10=pd.DataFrame(dust206_10.iloc[0,2:]).reset_index().rename(\n",
    "        columns={'index':'broadDay',0:'dust10'})\n",
    "    dust206_10['broadMonth']=6\n",
    "    dust202006=pd.concat([dust202006,(pd.merge(dust206_2,dust206_10,on=['broadMonth','broadDay']))])\n",
    "\n",
    "    dust202006['broadDay']=dust202006['broadDay'].str.rstrip('일')\n",
    "    dust202006['broadDay']=dust202006['broadDay'].astype('int')\n",
    "    dust202006['broadYear']=2020\n",
    "    dust202006.dropna(axis=0,inplace=True) # 없는 날짜가 존재하는 행 삭제\n",
    "\n",
    "    dust=pd.concat([dust2019,dust202006])\n",
    "\n",
    "    fc2 = []\n",
    "    for i in dust['dust2.5'].values:\n",
    "        if i <= 15:\n",
    "            fc2.append(1)\n",
    "        elif 16 <= i <= 35:\n",
    "            fc2.append(2)\n",
    "        elif 36 <= i <= 75:\n",
    "            fc2.append(3)\n",
    "        else:\n",
    "            fc2.append(4)\n",
    "\n",
    "    fc10 = []\n",
    "    for i in dust['dust10'].values:\n",
    "        if i <= 30:\n",
    "            fc10.append(1)\n",
    "        elif 31 <= i <= 80:\n",
    "            fc10.append(2)\n",
    "        elif 81 <= i <= 150:\n",
    "            fc10.append(3)\n",
    "        else:\n",
    "            fc10.append(4)\n",
    "\n",
    "    dustOrd = []\n",
    "    for i, j in zip(fc2, fc10):\n",
    "        dustOrd.append(max([i, j]))\n",
    "\n",
    "    # 카테고리로 변환\n",
    "    dustCat = []\n",
    "    for i in dustOrd:\n",
    "        if i == 1:\n",
    "            dustCat.append('좋음')\n",
    "        elif i == 2:\n",
    "            dustCat.append('보통')\n",
    "        elif i == 3:\n",
    "            dustCat.append('나쁨')\n",
    "        else:\n",
    "            dustCat.append('매우나쁨')\n",
    "\n",
    "    dust['dustCat'] = dustCat\n",
    "    dust = dust.drop(['dust2.5', 'dust10'], axis=1)\n",
    "\n",
    "    data=pd.merge(data,dust,on=['broadYear','broadMonth','broadDay'])\n",
    "\n",
    "    data.sort_values(by='no', inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    del data['no']\n",
    "    del data['broadYear']\n",
    "    del data['broadDateTime']\n",
    "    del data['broadMin']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "외부변수: 생활물가지수(전월대비 증감률), 기온(달에 대해서 정규화), 강수량(1년에 대해서 정규화), 미세먼지 지수를 <br> 넣어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방영달 -> 계절로 변화(봄, 여름, 가을, 겨울)\n",
    "def broadMonth_app(lst):\n",
    "    res = []\n",
    "    for item in lst:\n",
    "        if item in (3, 4, 5):\n",
    "            res.append(0)\n",
    "        elif item in (6, 7, 8):\n",
    "            res.append(1)\n",
    "        elif item in (9, 10, 11):\n",
    "            res.append(2)\n",
    "        else:\n",
    "            res.append(3)\n",
    "\n",
    "    return res\n",
    "\n",
    "# 방영 날짜: 1-10일이면 전반, 11-20이면 중반, 21-이면 후반\n",
    "def broadDay_app(lst):\n",
    "    res = []\n",
    "    for item in lst:\n",
    "        if item in range(1, 11):\n",
    "            res.append(0)\n",
    "        elif item in range(11, 21):\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(2)\n",
    "    return res\n",
    "\n",
    "# 적용\n",
    "def change_broad_times(data):\n",
    "    data['broadMonth'] = broadMonth_app(data['broadMonth'].values.tolist())\n",
    "    data['broadDay'] = broadDay_app(data['broadDay'].values.tolist())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방영달은 계절로 바꾸고, 방영날짜는 초/중/후반으로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 64\n",
    "\n",
    "names = {'전지현', '팽현숙', '전철우', '강레오', '김선영', '김정문', '김정배',\n",
    "         '김규흔', '이봉원', '오세득', '유귀열', '이경제', '이만기', '이보은', '임성근', '최인선',\n",
    "         '김병만', '김병지', '이정섭', '이동수', '서장훈', '송도순', '효재', '천수봉', '숀리',\n",
    "         '임화자', 'aab', 'aac'}\n",
    "\n",
    "# 제품명 정규화\n",
    "def regex_str(string: str):\n",
    "    string = string.lower()\n",
    "    string = re.sub('[^\\w\\s]', ' ', string)\n",
    "    string = re.sub('\\d+', ' ', string)\n",
    "    string = string.replace('여자', '여성').replace('남자', '남성')\n",
    "    string = string.replace('\\(무\\)', '무이자').replace('무\\)', '무이자')\n",
    "    string = string.replace('\\(일\\)', '일시불').replace('일\\)', '일시불')\n",
    "    string = re.sub('밥솥', ' 밥솥 ', string)\n",
    "    string = re.sub('침대', ' 침대 ', string)\n",
    "    string = re.sub('에어컨', ' 에어컨 ', string)\n",
    "    string = re.sub('노트북', ' 노트북 ', string)\n",
    "    string = re.sub('티셔츠', ' 티셔츠 ', string)\n",
    "    string = re.sub('스타일러', ' 스타일러 ', string)\n",
    "    string = re.sub('손질', ' 손질 ', string)\n",
    "    string = string.replace(\"s/s\",\"ss\").replace(\"ss\", \"시즌\")\n",
    "    string = string.replace(\"f/w\", \"시즌\").replace(\"썸머\", \"시즌\")\n",
    "    string = string.replace(\"lg\", \" lg \")\n",
    "    string = string.replace(\"울트라hd\", \" uhd \")\n",
    "    string = string.replace(\"tv\", \" tv \")\n",
    "    string = string.replace(\"김치\", \" 김치 \")\n",
    "    string = string.replace(\"기초세트\", \" 기초세트 \")\n",
    "    for name in names:\n",
    "        string = string.replace(name, ' ')\n",
    "    string = ' '.join([x for x in string.split() if len(x) > 1])\n",
    "    return string\n",
    "\n",
    "# corpus 만들기\n",
    "def make_test_corpus():\n",
    "    global test\n",
    "    test_prodName_corpus = set(' '.join(set(test['prodName'].apply(regex_str).values)).split())\n",
    "    return test_prodName_corpus\n",
    "\n",
    "\n",
    "test_prodName_corpus = make_test_corpus()\n",
    "\n",
    "\n",
    "def regex_str_train(string: str):\n",
    "    string = string.lower()\n",
    "    string = re.sub('[^\\w\\s]', ' ', string)\n",
    "    string = re.sub('\\d+', ' ', string)\n",
    "    string = string.replace('여자', '여성').replace('남자', '남성')\n",
    "    string = string.replace('\\(무\\)', '무이자').replace('무\\)', '무이자')\n",
    "    string = string.replace('\\(일\\)', '일시불').replace('일\\)', '일시불')\n",
    "    string = re.sub('밥솥', ' 밥솥 ', string)\n",
    "    string = re.sub('침대', ' 침대 ', string)\n",
    "    string = re.sub('에어컨', ' 에어컨 ', string)\n",
    "    string = re.sub('노트북', ' 노트북 ', string)\n",
    "    string = re.sub('티셔츠', ' 티셔츠 ', string)\n",
    "    string = re.sub('스타일러', ' 스타일러 ', string)\n",
    "    string = re.sub('손질', ' 손질 ', string)\n",
    "    string = string.replace(\"s/s\",\"ss\").replace(\"ss\", \"시즌\")\n",
    "    string = string.replace(\"f/w\", \"시즌\").replace(\"썸머\", \"시즌\")\n",
    "    string = string.replace(\"lg\", \" lg \")\n",
    "    string = string.replace(\"울트라hd\", \" uhd \")\n",
    "    string = string.replace(\"tv\", \" tv \")\n",
    "    string = string.replace(\"김치\", \" 김치 \")\n",
    "    string = string.replace(\"기초세트\", \" 기초세트 \")\n",
    "    for name in names:\n",
    "        string = string.replace(name, ' ')\n",
    "    string = ' '.join([x for x in string.split() if len(x) > 1 and x in test_prodName_corpus])\n",
    "    return string\n",
    "\n",
    "\n",
    "stopwords = {'gs', 'by', 'x', 'vbc', '봉', 'gabl', 'l', 'j',\n",
    "             'tq', 'a', 'un', 'the', 'dv', 'kna', 'nu', 'jk', '매',\n",
    "             'aab의', 'ih', 'ev', 'bna', '종', 'ia', 'kwa', 'kg',\n",
    "             'gablcrp', 'crp', 'in', 'hnc', '인용', 'g', 'af', 'um',\n",
    "             'fg', 'arc', 'nt', 'fq', 'b', 'uk', 'm', 'qs', '일시불', '무이자',\n",
    "             'wwj', 'pat', 'aae', 'fxkr', 'qv', 'fs', 'knb', '남성', '여성'} | names\n",
    "\n",
    "# tfidf vectorization\n",
    "def make_tfidf(data, flag=True):\n",
    "    if flag:\n",
    "        vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=150)\n",
    "        corpus = list(data['prodName'].apply(regex_str_train).values)\n",
    "        corpus.extend(list(test['prodName'].apply(regex_str).values))\n",
    "\n",
    "        vectorizer.fit(corpus)\n",
    "        train_tfidf = vectorizer.transform(data['prodName'].apply(regex_str_train)).toarray()\n",
    "\n",
    "        svd = TruncatedSVD(n_components=DIM, n_iter=7, random_state=42)\n",
    "        train_tfidf = svd.fit_transform(train_tfidf)\n",
    "        train_tfidf = pd.DataFrame(train_tfidf, columns=['Feature'+str(i) for i in range(DIM)])\n",
    "\n",
    "        data = pd.concat([data, train_tfidf], axis=1)\n",
    "\n",
    "        joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "    else:\n",
    "        vectorizer = joblib.load('vectorizer.pkl')\n",
    "        test_tfidf = vectorizer.transform(data['prodName'].apply(regex_str)).toarray()\n",
    "\n",
    "        svd = TruncatedSVD(n_components=DIM, n_iter=7, random_state=42)\n",
    "        test_tfidf = svd.fit_transform(test_tfidf)\n",
    "        test_tfidf = pd.DataFrame(test_tfidf, columns=['Feature' + str(i) for i in range(DIM)])\n",
    "\n",
    "        data = pd.concat([data, test_tfidf], axis=1)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트, 트레인 셋을 모두 이용하여 Tfidf Vectorizer를 만들어준 후 정규화된 제품명을 Tf-idf 벡터로 만듭니다. <br>\n",
    "그대로 쓰면 차원이 너무 커지기 때문에 SVD를 이용해서 64차원으로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgroup(data):\n",
    "    subgroup = pd.read_csv(\"../eda/data/merged_subGroup.csv\")\n",
    "    data = data.merge(subgroup, how = 'left')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NS홈쇼핑에서 가져온 소분류 카테고리를 데이터에 합쳐주는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상품군별로 스케일링\n",
    "def priceScaler(data, flag=True):\n",
    "    if flag:\n",
    "        data['unitPriceOrigin'] = data['unitPrice']\n",
    "\n",
    "        for group in data['prodGroup'].unique():\n",
    "            group_idx = data[data['prodGroup'] == group].index\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(data.loc[group_idx, 'unitPrice'].values.reshape(-1, 1))\n",
    "\n",
    "            data.loc[group_idx, 'unitPrice'] = scaler.transform(\n",
    "                data.loc[group_idx, 'unitPrice'].values.reshape(-1, 1))\n",
    "\n",
    "            file_name = '../eda/data/scaler/scaler_{}.pkl'.format(group)\n",
    "            joblib.dump(scaler, file_name)\n",
    "\n",
    "    else:\n",
    "        data['unitPriceOrigin'] = data['unitPrice']\n",
    "\n",
    "        for group in data['prodGroup'].unique():\n",
    "            group_idx = data[data['prodGroup'] == group].index\n",
    "\n",
    "            file_name = '../eda/data/scaler/scaler_{}.pkl'.format(group)\n",
    "            scaler = joblib.load(file_name)\n",
    "\n",
    "            data.loc[group_idx, 'unitPrice'] = scaler.transform(\n",
    "                data.loc[group_idx, 'unitPrice'].values.reshape(-1, 1))\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상품군별로 판매가격을 정규화해서 unitPrice를 바꿉니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 train preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data successfully preprocessed!\n"
     ]
    }
   ],
   "source": [
    "data = train\n",
    "data = add_newvars(data)\n",
    "data = change_dayofWeek(data)\n",
    "data = prodCount(data)\n",
    "data = change_time(data)\n",
    "data = holidays(data)\n",
    "data = add_index(data)\n",
    "data = isJune(data)\n",
    "data = stop(data)\n",
    "data = isNamed(data)\n",
    "data = isPayday(data)\n",
    "data = add_rating(data)\n",
    "data = add_exdata(data)\n",
    "data = change_broad_times(data)\n",
    "data = make_tfidf(data)\n",
    "data = subgroup(data)\n",
    "data = priceScaler(data)\n",
    "\n",
    "for col in ('stop', 'priceIndex', 'tempNorm', 'rainAvgWhole', 'rainAvgCap', 'target') + \\\n",
    "            tuple(['Feature' + str(i) for i in range(DIM)]):\n",
    "    data[col] = list(map(lambda x: round(x, 4), data[col].values.tolist()))\n",
    "\n",
    "print(\"Train data successfully preprocessed!\")\n",
    "train = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 만든 EDA 과정을 트레인 데이터에 적용해주고, computational overflow를 막기 위해 실수형 변수들을 <br>\n",
    "소숫점 4자리까지 반올림해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 test preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data successfully preprocessed!\n"
     ]
    }
   ],
   "source": [
    "data = test\n",
    "data = first_eda(data, flag=False)\n",
    "data = add_newvars(data)\n",
    "data = change_dayofWeek(data)\n",
    "data = prodCount(data)\n",
    "data = change_time(data)\n",
    "data = holidays(data)\n",
    "data = add_index(data)\n",
    "data = isJune(data, flag=False)\n",
    "data = stop(data)\n",
    "data = isNamed(data)\n",
    "data = isPayday(data)\n",
    "data = add_rating(data, flag=False)\n",
    "data = add_exdata(data)\n",
    "data = change_broad_times(data)\n",
    "data = make_tfidf(data, flag=False)\n",
    "data = subgroup(data)\n",
    "data = priceScaler(data, flag=False)\n",
    "\n",
    "for col in ('stop', 'priceIndex', 'tempNorm', 'rainAvgWhole', 'rainAvgCap', 'target') + \\\n",
    "            tuple(['Feature' + str(i) for i in range(DIM)]):\n",
    "    data[col] = list(map(lambda x: round(x, 4), data[col].values.tolist()))\n",
    "\n",
    "data[\"target\"] = np.nan\n",
    "data[\"revenue\"] = np.nan\n",
    "\n",
    "print(\"Test data successfully preprocessed!\")\n",
    "test = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 만든 EDA 과정을 테스트 데이터에 적용해주고, computational overflow를 막기 위해 실수형 변수들을 <br>\n",
    "소숫점 4자리까지 반올림해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "##### 나중에 지울거 ######\n",
    "\n",
    "train.to_csv(\"train.csv\")\n",
    "test.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling\n",
    "## 2.1 제품군 embedding 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    X = data.loc[:, 'Feature0':'Feature63']\n",
    "\n",
    "    prod = data['prodGroup'].unique()\n",
    "    prod = {val: i for i, val in enumerate(prod)}\n",
    "    func = lambda x: prod[x]\n",
    "    data['prodGroup'] = data['prodGroup'].apply(func).astype(int)\n",
    "    prod = data['subGroup'].unique()\n",
    "    prod = {val: i for i, val in enumerate(prod)}\n",
    "    func = lambda x: prod[x]\n",
    "    data['subGroup'] = data['subGroup'].apply(func).astype(int)\n",
    "\n",
    "    X1 = data['prodGroup']\n",
    "    X2 = data['subGroup']\n",
    "    X = pd.concat([X, X1, X2], axis=1)\n",
    "    y = data['target']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제품군(prodGroup)과 소분류 카테고리(subGroup - NS홈쇼핑에서 일일이 검색한 카테고리입니다.)를 정수값으로 변경합니다. <br> 이는 임베딩을 위해 필요한 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "EMBEDDING_DIM = DIM\n",
    "EPOCH = 300\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(3)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding dimension은 Tf-idf에서 정한 SVD의 차원인 64와 동일하게 정해줍니다. <br>\n",
    "왜냐하면 우리의 최종적인 임베딩은 prodGroup 임베딩, 소분류 임베딩, Tf-idf 임베딩을 Linear Projection한 것이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        # embedding1\n",
    "        self.x1 = torch.LongTensor(X_train.loc[:, X_train.columns == 'prodGroup'].values)\n",
    "        # embedding2\n",
    "        self.x2 = torch.LongTensor(X_train.loc[:, X_train.columns == 'subGroup'].values)\n",
    "        # embedding3\n",
    "        self.x3 = torch.FloatTensor(X_train.loc[:, \"Feature0\":\"Feature63\"].values)\n",
    "        self.y = torch.Tensor(y_train.values)\n",
    "\n",
    "    def __getitem__(self, s):\n",
    "        return self.x1[s], self.x2[s], self.x3[s], self.y[s]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "class TestData(TrainData):\n",
    "    '''Train 상속'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.emb1 = nn.Embedding(num_embeddings=11, embedding_dim=EMBEDDING_DIM)\n",
    "        self.emb2 = nn.Embedding(num_embeddings=44, embedding_dim=EMBEDDING_DIM)\n",
    "        self.enc1 = nn.Linear(EMBEDDING_DIM * 3, EMBEDDING_DIM, bias=False)\n",
    "        self.fc1_0 = nn.Linear(EMBEDDING_DIM, 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in [self.emb1.weight, self.emb2.weight, self.enc1.weight]:\n",
    "            torch.nn.init.kaiming_uniform_(layer)\n",
    "        for layer in [self.fc1_0.weight]:\n",
    "            torch.nn.init.kaiming_normal_(layer)\n",
    "        for layer in [self.fc1_0.bias]:\n",
    "            torch.nn.init.zeros_(layer)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1_emb = self.emb1(x1)\n",
    "        x1_emb = x1_emb.view(-1, x1_emb.shape[-1])\n",
    "        x2_emb = self.emb2(x2)\n",
    "        x2_emb = x2_emb.view(-1, x2_emb.shape[-1])\n",
    "        x_embs = self.enc1(torch.cat([x1_emb, x2_emb, x3], dim=1))\n",
    "        return self.fc1_0(x_embs), x_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 50, Train Loss: 0.8201319829055241\n",
      "Epoch: 100, Train Loss: 0.7937009323835373\n",
      "Epoch: 150, Train Loss: 0.7847264514082954\n",
      "Epoch: 200, Train Loss: 0.7801392961740494\n",
      "Epoch: 250, Train Loss: 0.7772836347443717\n",
      "Epoch: 300, Train Loss: 0.7753065173739478\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = preprocess(train)\n",
    "\n",
    "train_dataset = TrainData(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "net = NeuralNet().to(device)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "losses = []\n",
    "\n",
    "print(\"Start training...\")\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    net.train()\n",
    "    for x1, x2, x3, y in train_loader:\n",
    "        x1, x2, x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        y_pred, _ = net(x1, x2, x3)\n",
    "        y_pred = y_pred.view_as(y)\n",
    "        loss = criterion(y_pred, y)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {np.mean(losses)}\")\n",
    "\n",
    "torch.save(net.state_dict(), './nn_embs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done.\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "tmp = TrainData(X_train, y_train)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, train_embs = net(tmp.x1.to(device), tmp.x2.to(device), tmp.x3.to(device))\n",
    "    df = pd.DataFrame(train_embs.detach().cpu().numpy(), columns=['Feature'+str(i) for i in range(64)])\n",
    "    for col in df.columns:\n",
    "        df[col] = np.round(df[col], 4)\n",
    "    df.to_csv(\"../modeling/data/train_embs.csv\", index=False)\n",
    "\n",
    "    # test embs\n",
    "X_test, y_test = preprocess(test)\n",
    "tmp = TestData(X_test, y_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, test_embs = net(tmp.x1.to(device), tmp.x2.to(device), tmp.x3.to(device))\n",
    "    df = pd.DataFrame(test_embs.detach().cpu().numpy(), columns=['Feature' + str(i) for i in range(64)])\n",
    "    for col in df.columns:\n",
    "        df[col] = np.round(df[col], 4)\n",
    "    df.to_csv(\"../modeling/data/test_embs.csv\", index=False)\n",
    "\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩을 모두 훈련시킨후, 저장까지해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 각 모델에 피팅하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 데이터 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Lable_encoder###\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encoder(data):\n",
    "    encoder = LabelEncoder()\n",
    "    data['dustCat'] = encoder.fit_transform(data['dustCat'])\n",
    "    data['subGroup'] = encoder.fit_transform(data['subGroup'])\n",
    "    data['prodGroup'] = encoder.fit_transform(data['prodGroup'])\n",
    "\n",
    "    data[['dustCat', 'subGroup', 'prodGroup']] = data[['dustCat', 'subGroup', 'prodGroup']].astype('category')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dustCat의 원소가 한글이라 get_dummies를 인식못해서 따로 라벨을 붙혀줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_cate_xgb(data, final):\n",
    "    try:\n",
    "        data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(\"Original Shape : \", data.shape)\n",
    "\n",
    "    if final:\n",
    "        data.drop(['motherCode', 'prodCode', 'prodName', 'prodGroup', 'subGroup', 'isJune'], axis=1, inplace=True)\n",
    "    else:\n",
    "        data.drop(['motherCode', 'prodCode', 'prodName', 'prodGroup', 'subGroup'], axis=1, inplace=True)\n",
    "\n",
    "    broad_cat = ['broadMonth', 'broadDay', 'broadHour', 'broadDayOfWeek', 'holidayLen',\n",
    "                 'prod_index', 'max_index', 'dustCat']\n",
    "    for cat in broad_cat:\n",
    "        data[cat] = data[cat].astype('category')\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_cate_rf_lgbm(data, final):\n",
    "    try:\n",
    "        data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(\"Original Shape : \", data.shape)\n",
    "\n",
    "    if final:\n",
    "        data.drop(['motherCode', 'prodCode', 'prodName', 'isJune'], axis=1, inplace=True)\n",
    "    else:\n",
    "        data.drop(['motherCode', 'prodCode', 'prodName'], axis=1, inplace=True)\n",
    "\n",
    "    broad_cat = ['broadMonth', 'broadDay', 'broadHour', 'broadDayOfWeek', 'holidayLen',\n",
    "                 'prod_index', 'max_index', 'dustCat', 'prodGroup', 'subGroup']\n",
    "    for cat in broad_cat:\n",
    "        data[cat] = data[cat].astype('category')\n",
    "        \n",
    "    data = label_encoder(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_dummies 함수를 써서 카테고리 변수를 one-hot-encoding 하기 위해 변수들을 category 타입으로 지정해줍니다.\n",
    "- 단, start_index, end_index, isFemale, paymentPlan 변수는 0,1 로만 구성되어 있는 카테고리 변수이기 때문에 따로 카테고리 타입으로 지정해주지 않았습니다.\n",
    "- final(validation이 아닌 최종 train 및 test 예측일 때)이면 isJune 변수도 드랍합니다.\n",
    "- random forest 모델과 lgbm 모델은 prodGroup과 subGroup을 포함하고, xgboost는 두 변수를 빼고 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metric\n",
    "def MAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인 메트릭인 MAPE를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_dataclean(train, train_emb, isFinal, test = 0, test_emb = 0):\n",
    "    train_origin = train.copy()\n",
    "    train_xgb = make_dataset_cate_xgb(train, final = isFinal)\n",
    "    train_xgb.iloc[:, -65:-1] = train_emb\n",
    "    \n",
    "    if isFinal:\n",
    "        test_origin = test.copy()\n",
    "        test_xgb = make_dataset_cate_xgb(test, final = isFinal)\n",
    "        test.iloc[:, -65:-1] = test_emb\n",
    "        \n",
    "        return train_xgb, train_origin, test_xgb, test_origin\n",
    "        \n",
    "    else:\n",
    "        return train_xgb, train_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_lgb_dataclean(train, train_emb, isFinal, test = 0, test_emb = 0):\n",
    "    train_origin = train.copy()\n",
    "    train_rf_lgb = make_dataset_cate_rf_lgbm(train, final = isFinal)\n",
    "    train_rf_lgb.iloc[:, -66:-2] = train_emb \n",
    "    \n",
    "    if isFinal:\n",
    "        test_origin = test.copy()\n",
    "        test_rf_lgb = make_dataset_cate_rf_lgbm(test, final = isFinal)\n",
    "        test_rf_lgb.iloc[:, -66:-2] = test_emb\n",
    "        \n",
    "        return train_rf_lgb, train_origin, test_rf_lgb, test_origin\n",
    "        \n",
    "    else:\n",
    "        return train_rf_lgb, train_origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 모델 - XGBoost, RandomForest, LGBM\n",
    "- XGBoost, LGBM : Bayesian Optimization 사용\n",
    "- RandomForest : Random Search 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bayes_opt import BayesianOptimization\n",
    "import lightgbm as lgb \n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost가 hyperparameter tuning을 할 때 사용할 parameter의 범위\n",
    "xgb_pbounds = {'max_depth': (5, 10),\n",
    "           'learning_rate': (0.001, 0.2),\n",
    "           'n_estimators': (100, 600),\n",
    "           'gamma': (1., 0.001),\n",
    "           'min_child_weight': (2, 10),\n",
    "           'max_delta_step': (0, 0.1),\n",
    "           'subsample': (0.5, 1),\n",
    "           'colsample_bytree': (0.5, 0.99)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest가 hyperparameter tuning을 할 때 사용할 parameter의 범위\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "rf_pbounds = {'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 1500, num = 8)],\n",
    "               'max_features': ['sqrt','auto'],\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm이 hyperparameter tuning을 할 때 사용할 parameter의 범위\n",
    "lgbm_pbounds = {'num_leaves': (16, 300),\n",
    "          'feature_fraction': (0.1, 0.9),\n",
    "          'bagging_fraction': (0.8, 1),\n",
    "           'max_depth': (5,30),\n",
    "          'min_split_gain': (0.001, 0.1),\n",
    "          'min_child_weight': (30,50),\n",
    "          'learning_rate': (0.001, 0.2),  \n",
    "          'n_estimators': (16, 300),      \n",
    "          'subsample': (0, 1),          \n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2.1 Validation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_xgb(train):\n",
    "    X=train.drop(['target'], axis=1)\n",
    "    y=train['target']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,\n",
    "                                                      random_state=3, stratify= X.isJune)\n",
    "\n",
    "    X_train.drop(\"isJune\", axis = 1 ,inplace = True)\n",
    "    X_test.drop(\"isJune\", axis = 1, inplace = True)\n",
    "    \n",
    "    X_train=pd.get_dummies(X_train,drop_first=True)\n",
    "    X_test=pd.get_dummies(X_test,drop_first=True)\n",
    "\n",
    "    X__train=X_train.drop(['revenue','unitPriceOrigin'],axis=1)\n",
    "    X__test=X_test.drop(['revenue','unitPriceOrigin'],axis=1)\n",
    "    \n",
    "    return X__train, X__test, y_train, y_test, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_rf_lgbm(train):\n",
    "    X = train.drop('target',axis=1)\n",
    "    y = train['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
    "                                                        random_state=3, stratify=X.isJune)\n",
    "    \n",
    "    X__train = X_train.drop(['isJune','revenue', 'unitPriceOrigin'],axis=1)\n",
    "    X__test = X_test.drop(['isJune','revenue', 'unitPriceOrigin'], axis=1)\n",
    "    \n",
    "    return X__train, X__test, y_train, y_test, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_valid(train, param_search):\n",
    "    \n",
    "    print(\"---------------[XGBoost Model]---------------\")\n",
    "    print(\"Train test set split!\\n\")\n",
    "    \n",
    "    X__train, X__test, y_train, y_test, X_test = train_valid_xgb(train)\n",
    "\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    def XGB_cv(max_depth, learning_rate, n_estimators, gamma,\n",
    "               min_child_weight, max_delta_step, subsample,\n",
    "               colsample_bytree, nthread=-1):\n",
    "        model = XGBRegressor(max_depth=int(max_depth),\n",
    "                             learning_rate=learning_rate,\n",
    "                             n_estimators=int(n_estimators),\n",
    "                             nthread=nthread,\n",
    "                             gamma=gamma,\n",
    "                             min_child_weight=min_child_weight,\n",
    "                             max_delta_step=max_delta_step,\n",
    "                             subsample=subsample,\n",
    "                             colsample_bytree=colsample_bytree)\n",
    "        MAE = cross_val_score(model, X__train, y_train, scoring='neg_mean_absolute_error', cv = 5).mean()\n",
    "        return MAE\n",
    "    \n",
    "    print(\"Column : \\n\", X__train.columns)\n",
    "    print(\"\\nTrain X shape : \", X__train.shape)\n",
    "    print(\"Train Y sahpe : \", y_train.shape)\n",
    "    print(\"Test X shape : \", X__test.shape)\n",
    "    print(\"\\nBayes Optimization start!\")\n",
    "    \n",
    "    if param_search: # Bayes Optimization으로 hyperparameter tuning 진행\n",
    "        xgboostBO = BayesianOptimization(f=XGB_cv, pbounds=xgb_pbounds, verbose=2, random_state=3)\n",
    "        xgboostBO.maximize(init_points=5, n_iter=45)\n",
    "        xgboostBO = xgboostBO.max\n",
    "    \n",
    "    else: # hyperparameter tuning으로 찾아진 hyperparameter 값 바로 사용\n",
    "        xgboostBO = {'target': -0.31514963452668143, 'params': {'colsample_bytree': 0.561134482078139, 'gamma': 0.001,\n",
    "                                                            'learning_rate': 0.18329806043912528, 'max_delta_step': 0.06863918133671187,\n",
    "                                                            'max_depth': 9.980132921667362, 'min_child_weight': 9.928157427363239,\n",
    "                                                            'n_estimators': 521.4135613718304, 'subsample': 0.9468597761420319}}\n",
    "        \n",
    "    print(\"\\nBEST params : \", xgboostBO)\n",
    "    fit_xgb = XGBRegressor(max_depth=int(xgboostBO['params']['max_depth']),\n",
    "                   learning_rate=xgboostBO['params']['learning_rate'],\n",
    "                   n_estimators=int(xgboostBO['params']['n_estimators']),\n",
    "                   gamma=xgboostBO['params']['gamma'],\n",
    "                   min_child_weight=xgboostBO['params']['min_child_weight'],\n",
    "                   max_delta_step=xgboostBO['params']['max_delta_step'],\n",
    "                   subsample=xgboostBO['params']['subsample'],\n",
    "                   colsample_bytree=xgboostBO['params']['colsample_bytree'])\n",
    "        \n",
    "    xgb_model = fit_xgb.fit(X__train, y_train)\n",
    "\n",
    "    y_test2 = X_test['revenue']\n",
    "    y_pred = fit_xgb.predict(X__test)\n",
    "    final_pred = np.exp(y_pred) * X_test['broadTime'].astype('float') * X_test['unitPriceOrigin']\n",
    "\n",
    "    mape = MAPE(y_test2, final_pred)\n",
    "    print('MAPE:', mape)\n",
    "    \n",
    "    return fit_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_valid(train, param_search):\n",
    "    X__train, X__test, y_train, y_test, X_test = train_valid_rf_lgbm(train)\n",
    "    \n",
    "    print(\"---------------[Random Forest Model]---------------\")\n",
    "    print(\"Train test set split!\\n\")\n",
    "\n",
    "    np.random.seed(3)\n",
    "    rf = RandomForestRegressor(random_state=3)\n",
    "    cv = KFold(5, shuffle=True, random_state=3)\n",
    "    \n",
    "    print(\"Column : \\n\", X__train.columns)\n",
    "    print(\"\\nTrain X shape : \", X__train.shape)\n",
    "    print(\"Train Y sahpe : \", y_train.shape)\n",
    "    print(\"Test X shape : \", X__test.shape)\n",
    "    print(\"\\nRandom Search start!\")\n",
    "    \n",
    "    if param_search: # Random Search로 hyperparameter tuning 진행\n",
    "        rfgrid_1 = RandomizedSearchCV(rf, rf_pbounds, cv = cv, scoring= 'neg_mean_squared_error',\n",
    "                                  n_jobs = 7, return_train_score=True)\n",
    "        \n",
    "        bestParams = rfgrid_1.best_params_\n",
    "   \n",
    "    else: # hyperparameter tuning으로 찾아진 hyperparameter 값 바로 사용\n",
    "        bestParams = {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, \n",
    "                     'max_features': 'auto', 'max_depth': 76, 'bootstrap': True}\n",
    "        \n",
    "    print(\"\\nBEST params : \", bestParams)\n",
    "        \n",
    "    fit_rf = RandomForestRegressor(n_estimators=bestParams['n_estimators'],\n",
    "                                   max_depth=bestParams['max_depth'],\n",
    "                                   min_samples_split=bestParams['min_samples_split'],\n",
    "                                   min_samples_leaf=bestParams['min_samples_leaf'],\n",
    "                                   bootstrap=bestParams['bootstrap'],\n",
    "                                   max_features=bestParams['max_features'], random_state=3)\n",
    "        \n",
    "    rf_model = fit_rf.fit(X__train, y_train)\n",
    "\n",
    "    y_test2 = X_test['revenue']\n",
    "    y_pred = fit_rf.predict(X__test)\n",
    "    final_pred = np.exp(y_pred) * X_test['broadTime'].astype('float') * X_test['unitPriceOrigin']\n",
    "\n",
    "    mape = MAPE(y_test2, final_pred)\n",
    "    print('MAPE:', mape)\n",
    "    \n",
    "    return fit_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_valid(train, param_search):\n",
    "    \n",
    "    print(\"---------------[Light GBM Model]---------------\")\n",
    "    print(\"Train test set split!\\n\")\n",
    "    \n",
    "    X__train, X__test, y_train, y_test, X_test = train_valid_rf_lgbm(train)\n",
    "\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    def lgb_cv(num_leaves, feature_fraction, bagging_fraction, \n",
    "           max_depth, min_split_gain, min_child_weight,\n",
    "           learning_rate, n_estimators, subsample):\n",
    "        model = lgb.LGBMRegressor(\n",
    "            num_leaves = int(round(num_leaves)),\n",
    "            feature_fraction = max(min(feature_fraction, 1), 0),\n",
    "            bagging_fraction = max(min(bagging_fraction, 1), 0),\n",
    "            max_depth = int(round(max_depth)),\n",
    "            min_split_gain = min_split_gain,\n",
    "            min_child_weight = min_child_weight,\n",
    "            learning_rate = learning_rate, \n",
    "            n_estimators = int(round(n_estimators)),\n",
    "            subsample = np.clip(subsample, 0, 1))\n",
    "\n",
    "        MAE = cross_val_score(\n",
    "            model, X__train.values, y_train.values, \n",
    "            scoring='neg_mean_absolute_error', cv=5).mean()\n",
    "        return MAE\n",
    "    \n",
    "    print(\"Column : \\n\", X__train.columns)\n",
    "    print(\"\\nTrain X shape : \", X__train.shape)\n",
    "    print(\"Train Y sahpe : \", y_train.shape)\n",
    "    print(\"Test X shape : \", X__test.shape)\n",
    "    print(\"\\nBayes Optimization start!\")\n",
    "    \n",
    "    if param_search: # Bayes Optimization으로 hyperparameter tuning 진행\n",
    "        lgbBO = BayesianOptimization(f=lgb_cv, pbounds=lgbm_pbounds, verbose=2,random_state=3)\n",
    "        lgbBO.maximize(init_points=5, n_iter = 45)\n",
    "        bestParams = lgbBO.max['params']\n",
    "\n",
    "    else: # hyperparameter tuning으로 찾아진 hyperparameter 값 바로 사용\n",
    "        bestParams = {'bagging_fraction': 0.8119808750206314, 'feature_fraction': 0.8799983013520603, \n",
    "                      'learning_rate': 0.13556614571375009, 'max_depth': 18.806832590144996, 'min_child_weight': 39.25971967342293, \n",
    "                      'min_split_gain': 0.008664144839057038, 'n_estimators': 250.6692179822859, \n",
    "                      'num_leaves': 105.13032646164234, 'subsample': 0.861280669085226}\n",
    "        \n",
    "    print(\"BEST params : \", bestParams)  \n",
    "    fit_lgbm = lgb.LGBMRegressor(num_leaves = int(round(bestParams['num_leaves'])),\n",
    "                                           feature_fraction = bestParams['feature_fraction'],\n",
    "                                           bagging_fraction = bestParams['bagging_fraction'],\n",
    "                                           max_depth = int(round(bestParams['max_depth'])),\n",
    "                                           min_split_gain = bestParams['min_split_gain'],\n",
    "                                           min_child_weight = bestParams['min_child_weight'],\n",
    "                                           learning_rate = bestParams['learning_rate'],\n",
    "                                           n_estimators = int(round(bestParams['n_estimators'])),\n",
    "                                           subsample = bestParams['subsample'],\n",
    "                                           random_state=3)\n",
    "    model = fit_lgbm.fit(X__train, y_train)\n",
    "\n",
    "    y_test2 = X_test['revenue']\n",
    "    y_pred = fit_lgbm.predict(X__test)\n",
    "    final_pred = np.exp(y_pred) * X_test['broadTime'].astype('float') * X_test['unitPriceOrigin']\n",
    "\n",
    "    mape = MAPE(y_test2, final_pred)\n",
    "    print('MAPE:', mape)\n",
    "    \n",
    "    return fit_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = pd.read_csv(\"../modeling/data/train_embs.csv\")\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test_emb = pd.read_csv(\"../modeling/data/test_embs.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_xgb_data, train_valid_rf_lgb_data = train.copy(), train.copy()\n",
    "train_final_xgb_data, train_final_rf_lgb_data = train.copy(), train.copy()\n",
    "test_final_xgb_data, test_final_rf_lgb_data = test.copy(), test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape :  (35379, 96)\n",
      "---------------[XGBoost Model]---------------\n",
      "Train test set split!\n",
      "\n",
      "Column : \n",
      " Index(['broadTime', 'unitPrice', 'isFemale', 'paymentPlan', 'prodCount',\n",
      "       'start_index', 'end_index', 'stop', 'isNamed', 'isPayday',\n",
      "       ...\n",
      "       'max_index_5', 'max_index_6', 'max_index_7', 'max_index_8',\n",
      "       'max_index_9', 'max_index_10', 'max_index_12', 'dustCat_매우나쁨',\n",
      "       'dustCat_보통', 'dustCat_좋음'],\n",
      "      dtype='object', length=139)\n",
      "\n",
      "Train X shape :  (26534, 139)\n",
      "Train Y sahpe :  (26534,)\n",
      "Test X shape :  (8845, 139)\n",
      "\n",
      "Bayes Optimization start!\n",
      "\n",
      "BEST params :  {'target': -0.31514963452668143, 'params': {'colsample_bytree': 0.561134482078139, 'gamma': 0.001, 'learning_rate': 0.18329806043912528, 'max_delta_step': 0.06863918133671187, 'max_depth': 9.980132921667362, 'min_child_weight': 9.928157427363239, 'n_estimators': 521.4135613718304, 'subsample': 0.9468597761420319}}\n",
      "[02:38:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "MAPE: 35.0760124090276\n"
     ]
    }
   ],
   "source": [
    "# xgb\n",
    "train_xgb, train_origin_xgb = xgb_dataclean(train_valid_xgb_data, train_emb, isFinal = False)\n",
    "xgb_valid_model = xgb_valid(train_xgb, param_search = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape :  (35379, 96)\n"
     ]
    }
   ],
   "source": [
    "# rf & lgbm\n",
    "train_rf, train_origin_rf = rf_lgb_dataclean(train_valid_rf_lgb_data, train_emb, isFinal = False)\n",
    "train_lgbm, train_origin_lgbm = train_rf.copy(), train_origin_rf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------[Random Forest Model]---------------\n",
      "Train test set split!\n",
      "\n",
      "Column : \n",
      " Index(['broadMonth', 'broadDay', 'broadHour', 'broadTime', 'prodGroup',\n",
      "       'unitPrice', 'isFemale', 'paymentPlan', 'broadDayOfWeek', 'prodCount',\n",
      "       'holidayLen', 'prod_index', 'max_index', 'start_index', 'end_index',\n",
      "       'stop', 'isNamed', 'isPayday', 'rating', 'priceIndex', 'tempNorm',\n",
      "       'rainAvgWhole', 'rainAvgCap', 'dustCat', 'Feature0', 'Feature1',\n",
      "       'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7',\n",
      "       'Feature8', 'Feature9', 'Feature10', 'Feature11', 'Feature12',\n",
      "       'Feature13', 'Feature14', 'Feature15', 'Feature16', 'Feature17',\n",
      "       'Feature18', 'Feature19', 'Feature20', 'Feature21', 'Feature22',\n",
      "       'Feature23', 'Feature24', 'Feature25', 'Feature26', 'Feature27',\n",
      "       'Feature28', 'Feature29', 'Feature30', 'Feature31', 'Feature32',\n",
      "       'Feature33', 'Feature34', 'Feature35', 'Feature36', 'Feature37',\n",
      "       'Feature38', 'Feature39', 'Feature40', 'Feature41', 'Feature42',\n",
      "       'Feature43', 'Feature44', 'Feature45', 'Feature46', 'Feature47',\n",
      "       'Feature48', 'Feature49', 'Feature50', 'Feature51', 'Feature52',\n",
      "       'Feature53', 'Feature54', 'Feature55', 'Feature56', 'Feature57',\n",
      "       'Feature58', 'Feature59', 'Feature60', 'Feature61', 'Feature62',\n",
      "       'Feature63', 'subGroup'],\n",
      "      dtype='object')\n",
      "\n",
      "Train X shape :  (26534, 89)\n",
      "Train Y sahpe :  (26534,)\n",
      "Test X shape :  (8845, 89)\n",
      "\n",
      "Random Search start!\n",
      "\n",
      "BEST params :  {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 76, 'bootstrap': True}\n",
      "MAPE: 39.502647331015396\n"
     ]
    }
   ],
   "source": [
    "# rf\n",
    "rf_valid_model = rf_valid(train_rf, param_search = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------[Light GBM Model]---------------\n",
      "Train test set split!\n",
      "\n",
      "Column : \n",
      " Index(['broadMonth', 'broadDay', 'broadHour', 'broadTime', 'prodGroup',\n",
      "       'unitPrice', 'isFemale', 'paymentPlan', 'broadDayOfWeek', 'prodCount',\n",
      "       'holidayLen', 'prod_index', 'max_index', 'start_index', 'end_index',\n",
      "       'stop', 'isNamed', 'isPayday', 'rating', 'priceIndex', 'tempNorm',\n",
      "       'rainAvgWhole', 'rainAvgCap', 'dustCat', 'Feature0', 'Feature1',\n",
      "       'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7',\n",
      "       'Feature8', 'Feature9', 'Feature10', 'Feature11', 'Feature12',\n",
      "       'Feature13', 'Feature14', 'Feature15', 'Feature16', 'Feature17',\n",
      "       'Feature18', 'Feature19', 'Feature20', 'Feature21', 'Feature22',\n",
      "       'Feature23', 'Feature24', 'Feature25', 'Feature26', 'Feature27',\n",
      "       'Feature28', 'Feature29', 'Feature30', 'Feature31', 'Feature32',\n",
      "       'Feature33', 'Feature34', 'Feature35', 'Feature36', 'Feature37',\n",
      "       'Feature38', 'Feature39', 'Feature40', 'Feature41', 'Feature42',\n",
      "       'Feature43', 'Feature44', 'Feature45', 'Feature46', 'Feature47',\n",
      "       'Feature48', 'Feature49', 'Feature50', 'Feature51', 'Feature52',\n",
      "       'Feature53', 'Feature54', 'Feature55', 'Feature56', 'Feature57',\n",
      "       'Feature58', 'Feature59', 'Feature60', 'Feature61', 'Feature62',\n",
      "       'Feature63', 'subGroup'],\n",
      "      dtype='object')\n",
      "\n",
      "Train X shape :  (26534, 89)\n",
      "Train Y sahpe :  (26534,)\n",
      "Test X shape :  (8845, 89)\n",
      "\n",
      "Bayes Optimization start!\n",
      "BEST params :  {'bagging_fraction': 0.8119808750206314, 'feature_fraction': 0.8799983013520603, 'learning_rate': 0.13556614571375009, 'max_depth': 18.806832590144996, 'min_child_weight': 39.25971967342293, 'min_split_gain': 0.008664144839057038, 'n_estimators': 250.6692179822859, 'num_leaves': 105.13032646164234, 'subsample': 0.861280669085226}\n",
      "MAPE: 33.537248011923324\n"
     ]
    }
   ],
   "source": [
    "# lgbm\n",
    "lgbm_valid_model = lgbm_valid(train_lgbm, param_search = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2.2 Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_final(train, test, isXgb):\n",
    "    if isXgb:\n",
    "        train_X = pd.get_dummies(train.drop(['target', 'revenue', 'unitPriceOrigin'], axis = 1))\n",
    "        test_X = pd.get_dummies(test.drop(['target', 'revenue', 'unitPriceOrigin'], axis = 1))\n",
    "\n",
    "    else:\n",
    "        train_X = train.drop(['target', 'unitPriceOrigin', 'revenue'], axis=1)\n",
    "        test_X = test.drop(['target','unitPriceOrigin','revenue'], axis=1)\n",
    "\n",
    "    train_Y = train['target']\n",
    "\n",
    "    test_column = list(test_X.columns)\n",
    "    for column in train_X.columns:\n",
    "        if column not in test_column:\n",
    "            test_X[column] = 0\n",
    "\n",
    "    test_X = test_X[train_X.columns]\n",
    "    \n",
    "    return train_X, train_Y, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_xgb(train, test, param_search):\n",
    "    \n",
    "    print(\"---------------[XGBoost Model]---------------\")\n",
    "    print(\"Train test set clean!\\n\")\n",
    "    \n",
    "    train_X, train_Y, test_X = train_test_final(train, test, isXgb = True)\n",
    "\n",
    "    np.random.seed(3)\n",
    "\n",
    "    print(\"Column : \\n\", train_X.columns)\n",
    "    print(\"Train X shape : \", train_X.shape)\n",
    "    print(\"Train Y sahpe : \", train_Y.shape)\n",
    "    print(\"Test X shape : \", test_X.shape)\n",
    "    print(\"XGBoost Bayes Optimization Start!\")\n",
    "    \n",
    "    if param_search:\n",
    "        def XGB_cv(max_depth, learning_rate, n_estimators, gamma\n",
    "                   , min_child_weight, max_delta_step, subsample\n",
    "                   , colsample_bytree, nthread=-1):\n",
    "            model = XGBRegressor(max_depth=int(max_depth),\n",
    "                                 learning_rate=learning_rate,\n",
    "                                 n_estimators=int(n_estimators),\n",
    "                                 nthread=nthread,\n",
    "                                 gamma=gamma,\n",
    "                                 min_child_weight=min_child_weight,\n",
    "                                 max_delta_step=max_delta_step,\n",
    "                                 subsample=subsample,\n",
    "                                 colsample_bytree=colsample_bytree)\n",
    "            MAE = cross_val_score(model, train_X, train_Y, scoring='neg_mean_absolute_error', cv=5).mean()\n",
    "            return MAE\n",
    "\n",
    "        xgboostBO = BayesianOptimization(f=XGB_cv, pbounds=pbounds, verbose=2, random_state=3)\n",
    "        xgboostBO.maximize(init_points=5, n_iter=45)\n",
    "        xgboostBO = xgboost.max\n",
    "        \n",
    "    else:\n",
    "        xgboostBO = {'target': -0.4231938004771939, 'params': {'colsample_bytree': 0.5922112179210803, 'gamma': 0.001,\n",
    "                                                               'learning_rate': 0.1674272151263528, 'max_delta_step': 0.0708801582700138,\n",
    "                                                               'max_depth': 8.975177509311667, 'min_child_weight': 9.110058896157721,\n",
    "                                                               'n_estimators': 588.5277619331036, 'subsample': 0.9857800068264405}}\n",
    "\n",
    "    print(\"BEST params : \", xgboostBO)\n",
    "\n",
    "    fit_xgb = XGBRegressor(max_depth=int(xgboostBO['params']['max_depth']),\n",
    "                           learning_rate=xgboostBO['params']['learning_rate'],\n",
    "                           n_estimators=int(xgboostBO['params']['n_estimators']),\n",
    "                           gamma=xgboostBO['params']['gamma'],\n",
    "                           min_child_weight=xgboostBO['params']['min_child_weight'],\n",
    "                           max_delta_step=xgboostBO['params']['max_delta_step'],\n",
    "                           subsample=xgboostBO['params']['subsample'],\n",
    "                           colsample_bytree=xgboostBO['params']['colsample_bytree'])\n",
    "\n",
    "    model = fit_xgb.fit(train_X, train_Y)\n",
    "\n",
    "    train_pred = fit_xgb.predict(train_X)\n",
    "    test_pred = fit_xgb.predict(test_X)\n",
    "\n",
    "    final_train_pred = np.exp(train_pred) * train['broadTime'].astype('float') * train['unitPriceOrigin']\n",
    "    final_test_pred = np.exp(test_pred) * test['broadTime'].astype('float') * test['unitPriceOrigin']\n",
    "\n",
    "    final_train_pred = pd.DataFrame(final_train_pred)\n",
    "    final_test_pred = pd.DataFrame(final_test_pred)\n",
    "\n",
    "    final_train_pred.to_csv(\"final_train_pred_xgb.csv\")\n",
    "    final_test_pred.to_csv(\"final_test_pred_xgb.csv\")\n",
    "\n",
    "    return final_train_pred, final_test_pred, fit_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_rf(train, test, param_search):\n",
    "    \n",
    "    print(\"---------------[Random Forest Model]---------------\")\n",
    "    print(\"Train test set clean!\\n\")\n",
    "    \n",
    "    train_X, train_Y, test_X = train_test_final(train, test, isXgb = False)\n",
    "\n",
    "    np.random.seed(3)\n",
    "    rf = RandomForestRegressor(random_state=3)\n",
    "    cv = KFold(5, shuffle=True, random_state=3)\n",
    "    \n",
    "    print(\"Column : \\n\", train_X.columns)\n",
    "    print(\"Train X shape : \", train_X.shape)\n",
    "    print(\"Train Y sahpe : \", train_Y.shape)\n",
    "    print(\"Test X shape : \", test_X.shape)\n",
    "    print(\"\\nRandom Search start!\")\n",
    "    \n",
    "    if param_search:\n",
    "        rfgrid_1 = RandomizedSearchCV(rf, rf_pbounds, cv = cv, scoring= 'neg_mean_squared_error',\n",
    "                                  n_jobs = 7, return_train_score=True)\n",
    "        \n",
    "        bestParams = rfgrid_1.best_params_\n",
    "   \n",
    "    else: \n",
    "        bestParams = {'n_estimators': 100, 'min_samples_split': 5,\n",
    "                      'min_samples_leaf': 2, 'max_features': 'auto',\n",
    "                      'max_depth': 76, 'bootstrap': True}\n",
    "        \n",
    "    print(\"\\nBEST params : \", bestParams)\n",
    "        \n",
    "    fit_rf = RandomForestRegressor(n_estimators=bestParams['n_estimators'],\n",
    "                                   max_depth=bestParams['max_depth'],\n",
    "                                   min_samples_split=bestParams['min_samples_split'],\n",
    "                                   min_samples_leaf=bestParams['min_samples_leaf'],\n",
    "                                   bootstrap=bestParams['bootstrap'],\n",
    "                                   max_features=bestParams['max_features'], random_state=3)\n",
    "        \n",
    "    np.random.seed(3)\n",
    "    \n",
    "    model = fit_rf.fit(train_X,train_Y)\n",
    "\n",
    "    train_pred = fit_rf.predict(train_X)\n",
    "    test_pred = fit_rf.predict(test_X)\n",
    "    \n",
    "    final_train_pred = np.exp(train_pred) * train['broadTime'].astype('float') * train['unitPriceOrigin']\n",
    "    final_test_pred = np.exp(test_pred) * test['broadTime'].astype('float') * test['unitPriceOrigin']\n",
    "\n",
    "    final_train_pred = pd.DataFrame(final_train_pred)\n",
    "    final_test_pred = pd.DataFrame(final_test_pred)\n",
    "\n",
    "    final_train_pred = pd.DataFrame(final_train_pred)\n",
    "    final_test_pred = pd.DataFrame(final_test_pred)\n",
    "\n",
    "    final_train_pred.to_csv(\"final_train_pred_rf.csv\")\n",
    "    final_test_pred.to_csv(\"final_test_pred_rf.csv\")\n",
    "\n",
    "    return final_train_pred, final_test_pred, fit_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_lgbm(train, test, param_search):\n",
    "    \n",
    "    print(\"---------------[Light GBM Model]---------------\")\n",
    "    print(\"Train test set clean!\\n\")\n",
    "    \n",
    "    train_X, train_Y, test_X = train_test_final(train, test, isXgb = False)\n",
    "\n",
    "    np.random.seed(3)\n",
    "\n",
    "    print(\"Column : \\n\", train_X.columns)\n",
    "    print(\"Train X shape : \", train_X.shape)\n",
    "    print(\"Train Y sahpe : \", train_Y.shape)\n",
    "    print(\"Test X shape : \", test_X.shape)\n",
    "    print(\"XGBoost Bayes Optimization Start!\")\n",
    "    \n",
    "    if param_search:\n",
    "        def lgb_cv(num_leaves, feature_fraction, bagging_fraction, \n",
    "                   max_depth, min_split_gain, min_child_weight,\n",
    "                   learning_rate, n_estimators, subsample):\n",
    "            model = lgb.LGBMRegressor(\n",
    "            num_leaves = int(round(num_leaves)),\n",
    "            feature_fraction = max(min(feature_fraction, 1), 0),\n",
    "            bagging_fraction = max(min(bagging_fraction, 1), 0),\n",
    "            max_depth = int(round(max_depth)),\n",
    "            min_split_gain = min_split_gain,\n",
    "            min_child_weight = min_child_weight,\n",
    "            learning_rate = learning_rate, \n",
    "            n_estimators = int(round(n_estimators)),\n",
    "            subsample = np.clip(subsample, 0, 1))\n",
    "\n",
    "            MAE = cross_val_score(model, X__train.values, y_train.values, \n",
    "                                  scoring='neg_mean_absolute_error', cv=5).mean()\n",
    "        return MAE\n",
    "    \n",
    "        lgbBO = BayesianOptimization(f=lgb_cv, pbounds=lgbm_pbounds, verbose=2,random_state=3)\n",
    "        lgbBO.maximize(init_points=5, n_iter = 45)\n",
    "        bestParmas = lgbBO.max['params']\n",
    "        \n",
    "    else:\n",
    "        bestParams = {'bagging_fraction': 0.9731589936660383, 'feature_fraction': 0.5757297248941633, \n",
    "                      'learning_rate': 0.06982196726597009, 'max_depth': 22.345555275557583, 'min_child_weight': 43.32592767723173,\n",
    "                      'min_split_gain': 0.044098125637682964, 'n_estimators': 172.83567350969471, 'num_leaves': 220.3220932675174,\n",
    "                      'subsample': 0.0270706243388118}\n",
    "\n",
    "    print(\"BEST params : \", bestParams)\n",
    "\n",
    "    fit_lgbm = lgb.LGBMRegressor(num_leaves = int(round(bestParams['num_leaves'])),\n",
    "                                 feature_fraction = bestParams['feature_fraction'],\n",
    "                                 bagging_fraction = bestParams['bagging_fraction'],\n",
    "                                 max_depth = int(round(bestParams['max_depth'])),\n",
    "                                 min_split_gain = bestParams['min_split_gain'],\n",
    "                                 min_child_weight = bestParams['min_child_weight'],\n",
    "                                 learning_rate = bestParams['learning_rate'], \n",
    "                                 n_estimators = int(round(bestParams['n_estimators'])), \n",
    "                                 subsample = bestParams['subsample'], \n",
    "                                 random_state=3)\n",
    "\n",
    "    np.random.seed(3)\n",
    "    model = fit_lgbm.fit(train_X,train_Y)\n",
    "\n",
    "    train_pred = fit_lgbm.predict(train_X)\n",
    "    test_pred = fit_lgbm.predict(test_X)\n",
    "    final_train_pred = np.exp(train_pred) * train['broadTime'].astype('float') * train['unitPriceOrigin']\n",
    "    final_test_pred = np.exp(test_pred) * test['broadTime'].astype('float') * test['unitPriceOrigin']\n",
    "\n",
    "    final_train_pred = pd.DataFrame(final_train_pred)\n",
    "    final_test_pred = pd.DataFrame(final_test_pred)\n",
    "\n",
    "    final_train_pred = pd.DataFrame(final_train_pred)\n",
    "    final_test_pred = pd.DataFrame(final_test_pred)\n",
    "\n",
    "    final_train_pred.to_csv(\"final_train_pred_lgbm.csv\")\n",
    "    final_test_pred.to_csv(\"final_test_pred_lgbm.csv\")\n",
    "\n",
    "    return final_train_pred, final_test_pred, fit_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV for ensemble weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_for_ensemble(model, data, data_origin, isXgb):\n",
    "    X = data.drop(['target'], axis=1)\n",
    "    y = data['target']\n",
    "    kf = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "    i = 0\n",
    "    \n",
    "    print(\"MODEL : \\n\", model)\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        if isXgb:\n",
    "            X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "            X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "        X__train = X_train.drop(['revenue', 'unitPriceOrigin'], axis=1)\n",
    "        X__test = X_test.drop(['revenue', 'unitPriceOrigin'], axis=1)\n",
    "\n",
    "        cv_model = model.fit(X__train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X__test)\n",
    "        cv_final_pred = np.exp(y_pred) * X_test['broadTime'].astype('float') * X_test['unitPriceOrigin']\n",
    "        X_test['pred'] = cv_final_pred\n",
    "        X_test['index'] = test_idx\n",
    "\n",
    "        if i==1 :\n",
    "            cv_merged = X_test\n",
    "        else:\n",
    "            cv_merged = pd.concat([cv_merged, X_test])\n",
    "\n",
    "        \n",
    "    print(\"X test shape : \", cv_merged.shape)\n",
    "    cv_merged = cv_merged.sort_values(by = ['index'])\n",
    "    cv_merged['prodGroup'] = data_origin['prodGroup']\n",
    "\n",
    "    mapes = []\n",
    "    for group in list(cv_merged[\"prodGroup\"].unique()):\n",
    "        train_group = cv_merged[cv_merged[\"prodGroup\"] == group]\n",
    "        new_mape = MAPE(train_group['revenue'], train_group['pred'])\n",
    "        mapes.append((group, new_mape))\n",
    "\n",
    "    for mape in mapes:\n",
    "        print(mape)\n",
    "        \n",
    "    return mapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape :  (35379, 96)\n",
      "Original Shape :  (2716, 96)\n",
      "---------------[XGBoost Model]---------------\n",
      "Train test set clean!\n",
      "\n",
      "Column : \n",
      " Index(['broadTime', 'unitPrice', 'isFemale', 'paymentPlan', 'prodCount',\n",
      "       'start_index', 'end_index', 'stop', 'isNamed', 'isPayday',\n",
      "       ...\n",
      "       'max_index_6', 'max_index_7', 'max_index_8', 'max_index_9',\n",
      "       'max_index_10', 'max_index_12', 'dustCat_나쁨', 'dustCat_매우나쁨',\n",
      "       'dustCat_보통', 'dustCat_좋음'],\n",
      "      dtype='object', length=147)\n",
      "Train X shape :  (35379, 147)\n",
      "Train Y sahpe :  (35379,)\n",
      "Test X shape :  (2716, 147)\n",
      "XGBoost Bayes Optimization Start!\n",
      "BEST params :  {'target': -0.4231938004771939, 'params': {'colsample_bytree': 0.5922112179210803, 'gamma': 0.001, 'learning_rate': 0.1674272151263528, 'max_delta_step': 0.0708801582700138, 'max_depth': 8.975177509311667, 'min_child_weight': 9.110058896157721, 'n_estimators': 588.5277619331036, 'subsample': 0.9857800068264405}}\n",
      "[02:50:54] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# xgb\n",
    "train_xgb, train_origin_xgb, test_xgb, test_origin_xgb = xgb_dataclean(train_final_xgb_data, train_emb, isFinal = True, test = test_final_xgb_data, test_emb = test_emb)\n",
    "xgb_train_pred, xgb_test_pred, xgb_final_model = final_xgb(train_xgb, test_xgb, param_search = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape :  (35379, 96)\n",
      "Original Shape :  (2716, 96)\n"
     ]
    }
   ],
   "source": [
    "# rf & lgbm\n",
    "train_rf, train_origin_rf, test_rf, test_origin_rf = rf_lgb_dataclean(train_final_rf_lgb_data, train_emb, isFinal = True, test = test_final_rf_lgb_data, test_emb = test_emb)\n",
    "train_lgbm, train_origin_lgbm, test_lgbm, test_origin_lgbm = train_rf.copy(), train_origin_rf.copy(), test_rf.copy(), test_origin_rf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------[Random Forest Model]---------------\n",
      "Train test set clean!\n",
      "\n",
      "Column : \n",
      " Index(['broadMonth', 'broadDay', 'broadHour', 'broadTime', 'prodGroup',\n",
      "       'unitPrice', 'isFemale', 'paymentPlan', 'broadDayOfWeek', 'prodCount',\n",
      "       'holidayLen', 'prod_index', 'max_index', 'start_index', 'end_index',\n",
      "       'stop', 'isNamed', 'isPayday', 'rating', 'priceIndex', 'tempNorm',\n",
      "       'rainAvgWhole', 'rainAvgCap', 'dustCat', 'Feature0', 'Feature1',\n",
      "       'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7',\n",
      "       'Feature8', 'Feature9', 'Feature10', 'Feature11', 'Feature12',\n",
      "       'Feature13', 'Feature14', 'Feature15', 'Feature16', 'Feature17',\n",
      "       'Feature18', 'Feature19', 'Feature20', 'Feature21', 'Feature22',\n",
      "       'Feature23', 'Feature24', 'Feature25', 'Feature26', 'Feature27',\n",
      "       'Feature28', 'Feature29', 'Feature30', 'Feature31', 'Feature32',\n",
      "       'Feature33', 'Feature34', 'Feature35', 'Feature36', 'Feature37',\n",
      "       'Feature38', 'Feature39', 'Feature40', 'Feature41', 'Feature42',\n",
      "       'Feature43', 'Feature44', 'Feature45', 'Feature46', 'Feature47',\n",
      "       'Feature48', 'Feature49', 'Feature50', 'Feature51', 'Feature52',\n",
      "       'Feature53', 'Feature54', 'Feature55', 'Feature56', 'Feature57',\n",
      "       'Feature58', 'Feature59', 'Feature60', 'Feature61', 'Feature62',\n",
      "       'Feature63', 'subGroup'],\n",
      "      dtype='object')\n",
      "Train X shape :  (35379, 89)\n",
      "Train Y sahpe :  (35379,)\n",
      "Test X shape :  (2716, 89)\n",
      "\n",
      "Random Search start!\n",
      "\n",
      "BEST params :  {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 76, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "# rf\n",
    "rf_train_pred, rf_test_pred, rf_final_model = final_rf(train_rf, test_rf, param_search = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------[Light GBM Model]---------------\n",
      "Train test set clean!\n",
      "\n",
      "Column : \n",
      " Index(['broadMonth', 'broadDay', 'broadHour', 'broadTime', 'prodGroup',\n",
      "       'unitPrice', 'isFemale', 'paymentPlan', 'broadDayOfWeek', 'prodCount',\n",
      "       'holidayLen', 'prod_index', 'max_index', 'start_index', 'end_index',\n",
      "       'stop', 'isNamed', 'isPayday', 'rating', 'priceIndex', 'tempNorm',\n",
      "       'rainAvgWhole', 'rainAvgCap', 'dustCat', 'Feature0', 'Feature1',\n",
      "       'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Feature7',\n",
      "       'Feature8', 'Feature9', 'Feature10', 'Feature11', 'Feature12',\n",
      "       'Feature13', 'Feature14', 'Feature15', 'Feature16', 'Feature17',\n",
      "       'Feature18', 'Feature19', 'Feature20', 'Feature21', 'Feature22',\n",
      "       'Feature23', 'Feature24', 'Feature25', 'Feature26', 'Feature27',\n",
      "       'Feature28', 'Feature29', 'Feature30', 'Feature31', 'Feature32',\n",
      "       'Feature33', 'Feature34', 'Feature35', 'Feature36', 'Feature37',\n",
      "       'Feature38', 'Feature39', 'Feature40', 'Feature41', 'Feature42',\n",
      "       'Feature43', 'Feature44', 'Feature45', 'Feature46', 'Feature47',\n",
      "       'Feature48', 'Feature49', 'Feature50', 'Feature51', 'Feature52',\n",
      "       'Feature53', 'Feature54', 'Feature55', 'Feature56', 'Feature57',\n",
      "       'Feature58', 'Feature59', 'Feature60', 'Feature61', 'Feature62',\n",
      "       'Feature63', 'subGroup'],\n",
      "      dtype='object')\n",
      "Train X shape :  (35379, 89)\n",
      "Train Y sahpe :  (35379,)\n",
      "Test X shape :  (2716, 89)\n",
      "XGBoost Bayes Optimization Start!\n",
      "BEST params :  {'bagging_fraction': 0.9731589936660383, 'feature_fraction': 0.5757297248941633, 'learning_rate': 0.06982196726597009, 'max_depth': 22.345555275557583, 'min_child_weight': 43.32592767723173, 'min_split_gain': 0.044098125637682964, 'n_estimators': 172.83567350969471, 'num_leaves': 220.3220932675174, 'subsample': 0.0270706243388118}\n"
     ]
    }
   ],
   "source": [
    "# lgbm\n",
    "lgbm_train_pred, lgbm_test_pred, lgbm_final_model = final_lgbm(train_lgbm, test_lgbm, param_search = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv_mapes = cv_for_ensemble(xgb_final_model, train_xgb, train_origin_xgb, isXgb = True)\n",
    "rf_cv_mapes = cv_for_ensemble(rf_final_model, train_rf, train_origin_rf, isXgb = False)\n",
    "lgbm_cv_mapes = cv_for_ensemble(lgbm_final_model, train_lgbm, train_origin_lgbm, isXgb = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_sum(xgb_mapes, rf_mapes, lgbm_mapes, data_origin, xgb_train_pred, rf_train_pred, lgbm_train_pred):\n",
    "    pred = xgb_train_pred\n",
    "    pred.rename(columns = {0 : 'xgb'}, inplace = True)\n",
    "    pred['rf'] = rf_train_pred\n",
    "    pred['lgbm'] = lgbm_train_pred\n",
    "    pred['final'] = 0\n",
    "    pred['prodGroup'] = data_origin['prodGroup']\n",
    "    xgb_inversed, rf_inversed, lgbm_inversed = [], [], []\n",
    "    \n",
    "    for i in range(len(xgb_cv_mapes)):\n",
    "        inverse = (1/(xgb_cv_mapes[i][1])) + (1/(rf_cv_mapes[i][1])) + (1/(lgbm_cv_mapes[i][1]))\n",
    "        xgb_inversed.append((1/(xgb_cv_mapes[i][1]))/inverse)\n",
    "        rf_inversed.append((1/(rf_cv_mapes[i][1]))/inverse)\n",
    "        lgbm_inversed.append((1/(lgbm_cv_mapes[i][1]))/inverse)\n",
    "        \n",
    "    print(\"\\nXGB Weights :\\n\", xgb_inversed)\n",
    "    print(\"\\nRF Weights :\\n\", rf_inversed)\n",
    "    print(\"\\nLGBM Weights :\\n\", lgbm_inversed)\n",
    "        \n",
    "    for i in range(len(xgb_cv_mapes)):\n",
    "        grp_name = xgb_cv_mapes[i][0]               \n",
    "        pred['final'][pred['prodGroup'] == grp_name] = ((pred['xgb'][pred['prodGroup'] == grp_name] * xgb_inversed[i])\n",
    "                                                         + (pred['rf'][pred['prodGroup'] == grp_name] * rf_inversed[i])\n",
    "                                                         + (pred['lgbm'][pred['prodGroup'] == grp_name] * xgb_inversed[i]))\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weighted = weight_sum(xgb_cv_mapes, rf_cv_mapes, lgbm_cv_mapes, train, xgb_train_pred, rf_train_pred, lgbm_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.08341855893393"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAPE(weighted['final'], train['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted['final'].to_csv(\"train_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGB Weights :\n",
      " [0.3405913905325114, 0.3397247621145796, 0.3401355604937333, 0.34717060958621326, 0.3435774887754052, 0.3430060753747281, 0.35564655006254176, 0.34997335475924474, 0.34470889560249723, 0.3376579523310813, 0.35290873190789884]\n",
      "\n",
      "RF Weights :\n",
      " [0.29841829453775626, 0.3105462107446997, 0.31143975130109547, 0.298065554850706, 0.3048148171050208, 0.3163881592949487, 0.2995096612932377, 0.2912489984574073, 0.30827970155119044, 0.319454491175293, 0.3021973053681798]\n",
      "\n",
      "LGBM Weights :\n",
      " [0.3609903149297324, 0.34972902714072074, 0.3484246882051713, 0.35476383556308055, 0.3516076941195739, 0.3406057653303233, 0.3448437886442207, 0.35877764678334795, 0.34701140284631227, 0.34288755649362573, 0.3448939627239213]\n"
     ]
    }
   ],
   "source": [
    "test_weighted = weight_sum(xgb_cv_mapes, rf_cv_mapes, lgbm_cv_mapes, train, xgb_test_pred, rf_test_pred, lgbm_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>final</th>\n",
       "      <th>prodGroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.311134e+06</td>\n",
       "      <td>1.052223e+07</td>\n",
       "      <td>6.976666e+06</td>\n",
       "      <td>8.687511e+06</td>\n",
       "      <td>의류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.559501e+07</td>\n",
       "      <td>1.952674e+07</td>\n",
       "      <td>1.417514e+07</td>\n",
       "      <td>1.596659e+07</td>\n",
       "      <td>의류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.480942e+07</td>\n",
       "      <td>3.053960e+07</td>\n",
       "      <td>2.516273e+07</td>\n",
       "      <td>2.953957e+07</td>\n",
       "      <td>의류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.565779e+07</td>\n",
       "      <td>1.710998e+07</td>\n",
       "      <td>1.558356e+07</td>\n",
       "      <td>1.574647e+07</td>\n",
       "      <td>의류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.576550e+07</td>\n",
       "      <td>2.983447e+07</td>\n",
       "      <td>2.728156e+07</td>\n",
       "      <td>2.697052e+07</td>\n",
       "      <td>의류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>6.205482e+06</td>\n",
       "      <td>9.761382e+06</td>\n",
       "      <td>9.769978e+06</td>\n",
       "      <td>8.473904e+06</td>\n",
       "      <td>주방</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>3.818743e+06</td>\n",
       "      <td>1.168152e+07</td>\n",
       "      <td>8.262836e+06</td>\n",
       "      <td>7.747466e+06</td>\n",
       "      <td>주방</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>8.105626e+06</td>\n",
       "      <td>1.195793e+07</td>\n",
       "      <td>9.084447e+06</td>\n",
       "      <td>9.571129e+06</td>\n",
       "      <td>주방</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>1.697284e+07</td>\n",
       "      <td>1.149075e+07</td>\n",
       "      <td>8.863133e+06</td>\n",
       "      <td>1.238857e+07</td>\n",
       "      <td>건강기능</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>2.275390e+07</td>\n",
       "      <td>2.264465e+07</td>\n",
       "      <td>1.493685e+07</td>\n",
       "      <td>1.978599e+07</td>\n",
       "      <td>건강기능</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2716 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               xgb            rf          lgbm         final prodGroup\n",
       "0     9.311134e+06  1.052223e+07  6.976666e+06  8.687511e+06        의류\n",
       "1     1.559501e+07  1.952674e+07  1.417514e+07  1.596659e+07        의류\n",
       "2     3.480942e+07  3.053960e+07  2.516273e+07  2.953957e+07        의류\n",
       "3     1.565779e+07  1.710998e+07  1.558356e+07  1.574647e+07        의류\n",
       "4     2.576550e+07  2.983447e+07  2.728156e+07  2.697052e+07        의류\n",
       "...            ...           ...           ...           ...       ...\n",
       "2711  6.205482e+06  9.761382e+06  9.769978e+06  8.473904e+06        주방\n",
       "2712  3.818743e+06  1.168152e+07  8.262836e+06  7.747466e+06        주방\n",
       "2713  8.105626e+06  1.195793e+07  9.084447e+06  9.571129e+06        주방\n",
       "2714  1.697284e+07  1.149075e+07  8.863133e+06  1.238857e+07      건강기능\n",
       "2715  2.275390e+07  2.264465e+07  1.493685e+07  1.978599e+07      건강기능\n",
       "\n",
       "[2716 rows x 5 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_weighted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-623312f94048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_pred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_weighted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'final'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_weighted' is not defined"
     ]
    }
   ],
   "source": [
    "final_pred_y = test_weighted['final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_y.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
